{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "from pvlib import clearsky, atmosphere\n",
    "from pvlib.location import Location\n",
    "import pvlib.irradiance as irrad\n",
    "from sklearn.neural_network import MLPRegressor \n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "#from pandas.tools.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import gauss\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn import linear_model\n",
    "#import cvxpy as cvp\n",
    "import pickle\n",
    "import json\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(x,y):\n",
    "    N = np.prod(x.shape)\n",
    "    return np.sqrt((np.sum(np.square(y-x)))/float(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiStepSARIMAforecast(curDf, model_fit, predHor):\n",
    "    \"\"\"\n",
    "    Obtain multiple-step ahead forecast using SARIMA model\n",
    "\n",
    "    curDf: the set of up-to-now collected observations\n",
    "    model_fit: fitted SARIMA model used for predictions\n",
    "    predHor: prediction horizon (set to what used in the MPC)\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = np.empty([1, predHor])  # array with multiple-step ahead predictions\n",
    "    curModel = SARIMAX(curDf, order=model_fit.specification.order,\n",
    "                       seasonal_order=model_fit.specification.seasonal_order)\n",
    "    #startTime = time.time()\n",
    "    curModelfit = curModel.filter(model_fit.params)\n",
    "    #print('It took {} seconds to build filter'.format(time.time()-startTime))\n",
    "\n",
    "    #startTime = time.time()\n",
    "    yhat = curModelfit.forecast(steps=predHor)\n",
    "    #print('It took {} seconds to forecast'.format(time.time() - startTime))\n",
    "    predictions = yhat.values\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiStepSARIMAforecast_withRetrain(curDf, model_fit, predHor, timeStep, retrainFlag):\n",
    "    \"\"\"\n",
    "    Obtain multiple-step ahead forecast using SARIMA model\n",
    "    Allows the possibility to retrain the model using predictions as 'new observations'\n",
    "\n",
    "    curDf: the set of up-to-now collected observations\n",
    "    model_fit: fitted SARIMA model used for predictions\n",
    "    predHor: prediction horizon (set to what used in the MPC)\n",
    "    timeStep: time step of forecasted time series (in minutes)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = np.empty([1, predHor])  # array with multiple-step ahead predictions\n",
    "    curDf_multistep = curDf  # auxiliary dataframe used for storing the previously generated predictions as \"new observations\"\n",
    "\n",
    "    curModel = SARIMAX(curDf, order=model_fit.specification.order,\n",
    "                       seasonal_order=model_fit.specification.seasonal_order)\n",
    "    curModelfit = curModel.filter(model_fit.params)\n",
    "\n",
    "    for i in range(predHor):\n",
    "        # 1-step ahead prediction\n",
    "        yhat = curModelfit.forecast(steps=1)\n",
    "        yhat_df = pd.DataFrame(data=np.array(yhat), index=[curDf_multistep.index[-1] + pd.Timedelta(minutes=timeStep)])\n",
    "        curDf_multistep = curDf_multistep.append(yhat_df)  # update auxiliary (local) prediction dataframe\n",
    "        predictions[0, i] = float(yhat.values)\n",
    "\n",
    "        # Record 1-step ahead prediction separately\n",
    "        if i == 0:\n",
    "            yhat_1step = pd.DataFrame(data=np.array(yhat), index=[curDf.index[-1] + pd.Timedelta(minutes=timeStep)])\n",
    "\n",
    "        # Update SARIMA model with observation proxies (previous model predictions) up to current time step within the prediction horizon\n",
    "        curModel = SARIMAX(curDf_multistep, order=model_fit.specification.order,\n",
    "                           seasonal_order=model_fit.specification.seasonal_order)\n",
    "        if (retrainFlag):\n",
    "            curModelfit = curModel.fit()\n",
    "        else:\n",
    "            curModelfit = curModel.filter(model_fit.params)\n",
    "\n",
    "    return predictions, yhat_1step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPVforecast_v1(obsDf,SARIMAorder,SARIMAparams,predHor,timeStep,timeStepCtrl,tsPeriod,retrainFlag,NNmodel,normalizeData,wfDf,alpha,resample2CtrlStep):   \n",
    "    '''\n",
    "    This function is called at each MPC execution to return the forecast of the uncertain disturbance\n",
    "    \n",
    "    Inputs:\n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    SARIMAorder: order of SARIMA model as (p,d,q,P,D,Q)\n",
    "    SARIMAparams: parameters of SARIMA model\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    timeStepCtrl: MPC control time step (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    NNmodel: object with the fitted Neural Network model\n",
    "    normalizeData: if True, I/O data are normalized; otherwise, they are not\n",
    "    wfDf: dataframe with necessary weather forecasts. Dataframe has only 1 row (latest forecast)\n",
    "    alpha: weighting factors to combine SARIMA and NN predictions. Final prediction=alpha*SARIMA+(1-alpha)*NN\n",
    "    resample2CtrlStep: if True, the forecast will be resampled to the controller time step\n",
    "    \n",
    "    Outputs:\n",
    "    predFinal: forecasted uncertain disturbance\n",
    "    '''\n",
    "    # Get SARIMA predictions\n",
    "    maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "    maxLag = np.amax([1, maxSeasLag])\n",
    "    SARIMAmodel = SARIMAX(obsDf[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                       seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "    SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)    \n",
    "    if retrainFlag:\n",
    "        predSARIMA = multiStepSARIMAforecast_withRetrain(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor, timeStep, retrainFlag)\n",
    "    else:\n",
    "        predSARIMA = multiStepSARIMAforecast(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "    predSARIMA = np.maximum(np.zeros(len(predSARIMA)),predSARIMA)\n",
    "    \n",
    "    # Get NN predictions\n",
    "    numUpSampleNN = int(60/timeStep)\n",
    "    NNinput = np.concatenate(wfDf[['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']])\n",
    "    if normalizeData:\n",
    "        NNinput = normalize(NNinput.reshape(1,-1))\n",
    "    else:\n",
    "        NNinput = NNinput.reshape(1,-1)\n",
    "    tmpPred1h = np.array(NNmodel.predict(NNinput))\n",
    "    \n",
    "    xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "    xp = np.arange(1,tmpPred1h.shape[1]+1,1/float(numUpSampleNN))\n",
    "    tmpPred = np.interp(xp,xp1h,tmpPred1h[0])  \n",
    "    predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "    predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "    \n",
    "    # Combine SARIMA and NN\n",
    "    predSARIMA.shape = (len(predSARIMA),1)\n",
    "    predNN.shape = (len(predNN),1)\n",
    "    alpha.shape = (len(alpha),1)\n",
    "    pred = np.multiply(alpha,predSARIMA) + np.multiply(1-alpha,predNN)\n",
    "    \n",
    "    # Resample from prediction time step to MPC time step\n",
    "    if resample2CtrlStep:\n",
    "        if timeStep>=timeStepCtrl:\n",
    "            if np.mod(timeStep,timeStepCtrl)==0:    \n",
    "                numUpSample = int(timeStep/timeStepCtrl)\n",
    "                predFinal = np.repeat(np.array(pred),numUpSample)\n",
    "            else:\n",
    "                raise ValueError('If prediction time step is larger than control time step, \\\n",
    "                prediction time step must be an integer multiple of control time step!')\n",
    "        else:\n",
    "            if np.mod(timeStepCtrl,timeStep)==0:    \n",
    "                numDownSample = int(timeStepCtrl/timeStep)\n",
    "                predFinal = np.mean(pred.reshape(-1,numDownSample),axis=1)\n",
    "            else:\n",
    "                raise ValueError('If control time step is larger than prediction time step, \\\n",
    "                control time step must be an integer multiple of prediction time step!')\n",
    "    else:\n",
    "        predFinal = pred\n",
    "    \n",
    "    predFinal.shape = (len(predFinal),1)\n",
    "    return predFinal, pred, predSARIMA, predNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPVforecast(obsDf,SARIMAorder,SARIMAparams,predHor,timeStep,timeStepCtrl,tsPeriod,retrainFlag,NNmodel,normalizeData,wfDf,alpha,resample2CtrlStep):   \n",
    "    '''\n",
    "    This function is called at each MPC execution to return the forecast of the uncertain disturbance\n",
    "    \n",
    "    ***Temporary version with normalization for SARIMA and NN.\n",
    "    \n",
    "    Inputs:\n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    SARIMAorder: order of SARIMA model as (p,d,q,P,D,Q)\n",
    "    SARIMAparams: parameters of SARIMA model\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    timeStepCtrl: MPC control time step (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    NNmodel: object with the fitted Neural Network model\n",
    "    normalizeData: if True, I/O data are normalized; otherwise, they are not\n",
    "    wfDf: dataframe with necessary weather forecasts. Dataframe has only 1 row (latest forecast)\n",
    "    alpha: weighting factors to combine SARIMA and NN predictions. Final prediction=alpha*SARIMA+(1-alpha)*NN\n",
    "    resample2CtrlStep: if True, the forecast will be resampled to the controller time step\n",
    "    \n",
    "    Outputs:\n",
    "    predFinal: forecasted uncertain disturbance\n",
    "    '''\n",
    "    \n",
    "    normalizeFactor = float(2626.1504040404043)\n",
    "    obsDf = obsDf/normalizeFactor\n",
    "    \n",
    "    # Get SARIMA predictions\n",
    "    maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "    maxLag = np.amax([1, maxSeasLag])\n",
    "    SARIMAmodel = SARIMAX(obsDf[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                       seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "    SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)    \n",
    "    if retrainFlag:\n",
    "        predSARIMA = multiStepSARIMAforecast_withRetrain(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor, timeStep, retrainFlag)\n",
    "    else:\n",
    "        predSARIMA = multiStepSARIMAforecast(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "    predSARIMA = np.maximum(np.zeros(len(predSARIMA)),predSARIMA)\n",
    "    predSARIMA = predSARIMA*normalizeFactor\n",
    "    \n",
    "    # Get NN predictions\n",
    "    numUpSampleNN = int(60/timeStep)\n",
    "    NNinput = np.concatenate(wfDf[['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']])\n",
    "    if normalizeData:\n",
    "        NNinput = normalize(NNinput.reshape(1,-1))\n",
    "    else:\n",
    "        NNinput = NNinput.reshape(1,-1)\n",
    "    tmpPred1h = np.array(NNmodel.predict(NNinput))\n",
    "    \n",
    "    xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "    xp = np.arange(1,tmpPred1h.shape[1]+1,1/float(numUpSampleNN))\n",
    "    tmpPred = np.interp(xp,xp1h,tmpPred1h[0])  \n",
    "    predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "    predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "    predNN = predNN*normalizeFactor\n",
    "    \n",
    "    # Combine SARIMA and NN\n",
    "    predSARIMA.shape = (len(predSARIMA),1)\n",
    "    predNN.shape = (len(predNN),1)\n",
    "    alpha.shape = (len(alpha),1)\n",
    "    pred = np.multiply(alpha,predSARIMA) + np.multiply(1-alpha,predNN)\n",
    "    \n",
    "    # Resample from prediction time step to MPC time step\n",
    "    if resample2CtrlStep:\n",
    "        if timeStep>=timeStepCtrl:\n",
    "            if np.mod(timeStep,timeStepCtrl)==0:    \n",
    "                numUpSample = int(timeStep/timeStepCtrl)\n",
    "                predFinal = np.repeat(np.array(pred),numUpSample)\n",
    "            else:\n",
    "                raise ValueError('If prediction time step is larger than control time step, \\\n",
    "                prediction time step must be an integer multiple of control time step!')\n",
    "        else:\n",
    "            if np.mod(timeStepCtrl,timeStep)==0:    \n",
    "                numDownSample = int(timeStepCtrl/timeStep)\n",
    "                predFinal = np.mean(pred.reshape(-1,numDownSample),axis=1)\n",
    "            else:\n",
    "                raise ValueError('If control time step is larger than prediction time step, \\\n",
    "                control time step must be an integer multiple of prediction time step!')\n",
    "    else:\n",
    "        predFinal = pred\n",
    "    \n",
    "    predFinal.shape = (len(predFinal),1)\n",
    "    return predFinal, pred, predSARIMA, predNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPVforecast_v2_betaVersion(obsDf,wfDf,models,predHor=96,timeStep=15,timeStepCtrl=5,tsPeriod=96,retrainFlag=False,resample2CtrlStep=False):   \n",
    "    '''    \n",
    "    This function is called at each MPC execution to return the forecast of the uncertain disturbance\n",
    "    \n",
    "    Inputs:\n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    models: a dictionary with the active models for forecasting\n",
    "        SARIMAorder: order of SARIMA model as (p,d,q,P,D,Q)\n",
    "        SARIMAparams: parameters of SARIMA model\n",
    "        NNmodel: object with the fitted Neural Network model\n",
    "        alpha: weighting factors to combine SARIMA and NN predictions. Final prediction=alpha*SARIMA+(1-alpha)*NN\n",
    "        normPowerCoeff: coefficient to normalize PV power data\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    timeStepCtrl: MPC control time step (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    wfDf: dataframe with necessary weather forecasts. Dataframe has only 1 row (latest forecast)\n",
    "    resample2CtrlStep: if True, the forecast will be resampled to the controller time step\n",
    "    \n",
    "    Outputs:\n",
    "    predFinal: forecasted uncertain variable (sampled in controller time step)\n",
    "    pred: forecasted uncertain variable (sampled in the time step of forecasting module) \n",
    "    predSARIMA: SARIMA forecast (sampled in the time step of forecasting module)\n",
    "    predNN: NN forecast (sampled in the time step of forecasting module)\n",
    "    '''\n",
    "    \n",
    "    # Get predictions from linear regression\n",
    "    if models['regression']['loaded']==True:\n",
    "        reg = linear_model.LinearRegression()\n",
    "        histStepsRegr = models['regression']['history']\n",
    "        predStepsRegr = models['regression']['prediction']\n",
    "        reg.fit(np.arange(0,histStepsRegr).reshape(-1, 1),np.array(obsDf[-histStepsRegr:]))\n",
    "        predRegr = reg.predict(np.arange(histStepsRegr,histStepsRegr+predStepsRegr).reshape(-1, 1))\n",
    "        predRegr = np.maximum(np.zeros(len(predRegr)),predRegr)\n",
    "        predRegr.shape = (len(predRegr),1)\n",
    "    else:\n",
    "        predRegr = np.nan\n",
    "    \n",
    "    # Get SARIMA predictions\n",
    "    if models['sarima']['loaded']==True:\n",
    "        SARIMAorder = models['sarima']['model']['SARIMAorder']\n",
    "        SARIMAparams = models['sarima']['model']['SARIMAparams']\n",
    "        normalizeFactor = float(models['sarima']['normPowerCoeff'])\n",
    "        obsDf = obsDf/normalizeFactor\n",
    "        \n",
    "        maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "        maxLag = np.amax([1, maxSeasLag])\n",
    "        SARIMAmodel = SARIMAX(obsDf[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                           seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "        SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)    \n",
    "        if retrainFlag:\n",
    "            predSARIMA = multiStepSARIMAforecast_withRetrain(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor, timeStep, retrainFlag)\n",
    "        else:\n",
    "            predSARIMA = multiStepSARIMAforecast(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "        predSARIMA = np.maximum(np.zeros(len(predSARIMA)),predSARIMA)\n",
    "        predSARIMA = predSARIMA*normalizeFactor\n",
    "        predSARIMA.shape = (len(predSARIMA),1)\n",
    "    else:\n",
    "        predSARIMA = np.nan\n",
    "    \n",
    "    # Get NN predictions\n",
    "    if models['nn']['loaded']==True:\n",
    "        NNmodel = models['nn']['model']\n",
    "        normalizeData = models['nn']['normInputData']\n",
    "        normalizeFactor = models['nn']['normPowerCoeff']\n",
    "        numUpSampleNN = int(60/timeStep)\n",
    "        NNinput = np.concatenate(wfDf[['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']])\n",
    "        if normalizeData:\n",
    "            NNinput = normalize(NNinput.reshape(1,-1))\n",
    "        else:\n",
    "            NNinput = NNinput.reshape(1,-1)\n",
    "        tmpPred1h = np.array(NNmodel.predict(NNinput))\n",
    "\n",
    "        xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "        xp = np.arange(1,tmpPred1h.shape[1]+1,1/float(numUpSampleNN))\n",
    "        tmpPred = np.interp(xp,xp1h,tmpPred1h[0])  \n",
    "        predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "        predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "        predNN = predNN*normalizeFactor\n",
    "        predNN.shape = (len(predNN),1)\n",
    "    else:\n",
    "        predNN = np.nan\n",
    "    \n",
    "    # Combine SARIMA and NN\n",
    "    if (models['alpha']['loaded']==True) & (models['sarima']['loaded']==True) & (models['nn']['loaded']==True):\n",
    "        alpha = models['alpha']['model']\n",
    "        alpha.shape = (len(alpha),1)\n",
    "        pred = np.multiply(alpha,predSARIMA) + np.multiply(1-alpha,predNN)\n",
    "    elif (models['sarima']['loaded']==True):\n",
    "        pred = predSARIMA\n",
    "    elif (models['nn']['loaded']==True):\n",
    "        pred = predNN\n",
    "    else:\n",
    "        raise ValueError('At least one forecast model must be loaded!')\n",
    "    \n",
    "    # Combine (SARIMA+NN) with regression model\n",
    "    if models['regression']['loaded']==True:\n",
    "        pred[0:predStepsRegr] = predRegr\n",
    "    \n",
    "    # Resample from prediction time step to MPC time step\n",
    "    if resample2CtrlStep:\n",
    "        if timeStep>=timeStepCtrl:\n",
    "            if np.mod(timeStep,timeStepCtrl)==0:    \n",
    "                numUpSample = int(timeStep/timeStepCtrl)\n",
    "                predFinal = np.repeat(np.array(pred),numUpSample)\n",
    "            else:\n",
    "                raise ValueError('If prediction time step is larger than control time step, \\\n",
    "                prediction time step must be an integer multiple of control time step!')\n",
    "        else:\n",
    "            if np.mod(timeStepCtrl,timeStep)==0:    \n",
    "                numDownSample = int(timeStepCtrl/timeStep)\n",
    "                predFinal = np.mean(pred.reshape(-1,numDownSample),axis=1)\n",
    "            else:\n",
    "                raise ValueError('If control time step is larger than prediction time step, \\\n",
    "                control time step must be an integer multiple of prediction time step!')\n",
    "    else:\n",
    "        predFinal = pred\n",
    "    \n",
    "    predFinal.shape = (len(predFinal),1)\n",
    "    return predFinal, pred, predSARIMA, predNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeInputNN(models,Xf,cols_df,predHor):\n",
    "    normFactTa = float(models['nn']['normTa'])\n",
    "    normFactCC = float(models['nn']['normCC'])\n",
    "    normFactCS = float(models['nn']['normCS'])\n",
    "    normFactPdminus1 = 1 # dataframe already contains normalized power data\n",
    "    \n",
    "    if len(Xf.shape)==1: # reshaping needed if normalization is used when getting new forecast\n",
    "        Xf.shape=(1,Xf.shape[0]) \n",
    "    \n",
    "    if models['nn']['architecture'] == 'scalar':\n",
    "        if cols_df == ['cloud_cover_forecast','clear_sky_forecast']:\n",
    "            Xf[:,0] = Xf[:,0]/normFactCC\n",
    "            Xf[:,1] = Xf[:,1]/normFactCS\n",
    "        elif cols_df == ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']:\n",
    "            Xf[:,0] = Xf[:,0]/normFactTa\n",
    "            Xf[:,1] = Xf[:,1]/normFactCC\n",
    "            Xf[:,2] = Xf[:,2]/normFactCS\n",
    "        elif cols_df == ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']:\n",
    "            Xf[:,0] = Xf[:,0]/normFactTa\n",
    "            Xf[:,1] = Xf[:,1]/normFactCC\n",
    "            Xf[:,2] = Xf[:,2]/normFactCS\n",
    "            Xf[:,3] = Xf[:,3]/normFactPdminus1       \n",
    "        if models['nn']['inputData'][-1]==True:\n",
    "            Xf[:,-1] = Xf[:,-1]/float(predHor)\n",
    "    else:\n",
    "        if cols_df == ['cloud_cover_forecast','clear_sky_forecast']:\n",
    "            Xf[:,0:predHor] = Xf[:,0:predHor]/normFactCC\n",
    "            Xf[:,predHor:2*predHor] = Xf[:,predHor:2*predHor]/normFactCS\n",
    "        elif cols_df == ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']:\n",
    "            Xf[:,0:predHor] = Xf[:,0:predHor]/normFactTa\n",
    "            Xf[:,predHor:2*predHor] = Xf[:,predHor:2*predHor]/normFactCC\n",
    "            Xf[:,2*predHor:3*predHor] = Xf[:,2*predHor:3*predHor]/normFactCS\n",
    "        elif cols_df == ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']:\n",
    "            Xf[:,0:predHor] = Xf[:,0:predHor]/normFactTa\n",
    "            Xf[:,predHor:2*predHor] = Xf[:,predHor:2*predHor]/normFactCC\n",
    "            Xf[:,2*predHor:3*predHor] = Xf[:,2*predHor:3*predHor]/normFactCS\n",
    "            Xf[:,3*predHor:4*predHor] = Xf[:,3*predHor:4*predHor]/normFactPdminus1       \n",
    "        if models['nn']['inputData'][-1]==True:\n",
    "            Xf[:,-predHor:] = Xf[:,-predHor:]/float(predHor)\n",
    "    \n",
    "    return Xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPVforecast_v2(obsDf,wfDf,models,predHor=96,timeStep=15,timeStepCtrl=5,tsPeriod=96,retrainFlag=False,resample2CtrlStep=False):   \n",
    "    '''    \n",
    "    This function is called at each MPC execution to return the forecast of the uncertain disturbance\n",
    "    \n",
    "    Inputs:\n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    models: a dictionary with the active models for forecasting\n",
    "        SARIMAorder: order of SARIMA model as (p,d,q,P,D,Q)\n",
    "        SARIMAparams: parameters of SARIMA model\n",
    "        NNmodel: object with the fitted Neural Network model\n",
    "        alpha: weighting factors to combine SARIMA and NN predictions. Final prediction=alpha*SARIMA+(1-alpha)*NN\n",
    "        normPowerCoeff: coefficient to normalize PV power data\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    timeStepCtrl: MPC control time step (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    wfDf: dataframe with necessary weather forecasts. Dataframe has only 1 row (latest forecast)\n",
    "    resample2CtrlStep: if True, the forecast will be resampled to the controller time step\n",
    "    \n",
    "    Outputs:\n",
    "    predFinal: forecasted uncertain variable (sampled in controller time step)\n",
    "    pred: forecasted uncertain variable (sampled in the time step of forecasting module) \n",
    "    predSARIMA: SARIMA forecast (sampled in the time step of forecasting module)\n",
    "    predNN: NN forecast (sampled in the time step of forecasting module)\n",
    "    '''\n",
    "    \n",
    "    # Get predictions from linear regression\n",
    "    if models['regression']['loaded']==True:\n",
    "        reg = linear_model.LinearRegression()\n",
    "        histStepsRegr = models['regression']['history']\n",
    "        predStepsRegr = models['regression']['prediction']\n",
    "        reg.fit(np.arange(0,histStepsRegr).reshape(-1, 1),np.array(obsDf[-histStepsRegr:]))\n",
    "        predRegr = reg.predict(np.arange(histStepsRegr,histStepsRegr+predStepsRegr).reshape(-1, 1))\n",
    "        predRegr = np.maximum(np.zeros(len(predRegr)),predRegr)\n",
    "        predRegr.shape = (len(predRegr),1)\n",
    "    else:\n",
    "        predRegr = np.nan\n",
    "    \n",
    "    # Get SARIMA predictions\n",
    "    if models['sarima']['loaded']==True:\n",
    "        obsDfSARIMA = obsDf.copy()\n",
    "        SARIMAorder = models['sarima']['model']['SARIMAorder']\n",
    "        SARIMAparams = models['sarima']['model']['SARIMAparams']\n",
    "        normalizeFactor = float(models['sarima']['normPowerCoeff'])\n",
    "        obsDfSARIMA = obsDfSARIMA/normalizeFactor\n",
    "        maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "        maxLag = np.amax([1, maxSeasLag])\n",
    "        SARIMAmodel = SARIMAX(obsDfSARIMA[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                           seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "        SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)    \n",
    "        if retrainFlag:\n",
    "            predSARIMA = multiStepSARIMAforecast_withRetrain(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor, timeStep, retrainFlag)\n",
    "        else:\n",
    "            predSARIMA = multiStepSARIMAforecast(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "        predSARIMA = np.maximum(np.zeros(len(predSARIMA)),predSARIMA)\n",
    "        predSARIMA = predSARIMA*normalizeFactor\n",
    "        predSARIMA.shape = (len(predSARIMA),1)\n",
    "    else:\n",
    "        predSARIMA = np.nan\n",
    "    \n",
    "    # Get NN predictions\n",
    "    if models['nn']['loaded']==True:\n",
    "        obsDfNN = obsDf.copy()\n",
    "        NNmodel = models['nn']['model']\n",
    "        normalizeData = models['nn']['normInputData']\n",
    "        normalizeFactor = models['nn']['normPowerCoeff']\n",
    "        numUpSampleNN = int(60/timeStep)\n",
    "        predHorNN = 23\n",
    "        obsDfNN = obsDfNN/normalizeFactor\n",
    "        \n",
    "        # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "        if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "            cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df = comb_df[cols_df]\n",
    "            NNinput = np.concatenate(comb_df)\n",
    "        elif (models['nn']['inputData']==[True,True,True,False,False]) | (models['nn']['inputData']==[True,True,True,False,True]):\n",
    "            cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df = comb_df[cols_df]\n",
    "            NNinput = np.concatenate(comb_df)\n",
    "        elif (models['nn']['inputData']==[True,True,True,True,False]) | (models['nn']['inputData']==[True,True,True,True,True]):\n",
    "            cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "            obsDfdminus1 = obsDfNN.loc[obsDfNN.index>obsDfNN.index[-1]-pd.Timedelta('1 days 00:00:00')]\n",
    "            obsDfdminus1 = obsDfdminus1.resample('60T').mean()\n",
    "            obsDfdminus1 = obsDfdminus1.tolist()\n",
    "            obsDfdminus1 = obsDfdminus1[0:predHorNN]\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df['Observations_dminus1'] = obsDfdminus1\n",
    "            NNinput = np.concatenate(comb_df[cols_df])\n",
    "            \n",
    "        if models['nn']['architecture'] == 'scalar':\n",
    "            ncolsX = len(cols_df)\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "            Xf = np.empty([predHorNN,ncolsX])\n",
    "            Yf = np.empty([predHorNN,1])\n",
    "            idx = 0\n",
    "            for j in range(predHorNN):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(comb_df[k][j])]\n",
    "                if models['nn']['inputData'][-1]==True:\n",
    "                    tmp = tmp + [np.array(j+1)]\n",
    "                Xf[idx,:] = tmp\n",
    "                if normalizeData: \n",
    "                    Xf[idx,:] = normalize(Xf[idx,:].reshape(1,-1))\n",
    "                Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                idx += 1\n",
    "            tmpPred1h = np.array(Yf)\n",
    "            tmpPred1h.shape=(1,len(tmpPred1h))\n",
    "        elif models['nn']['architecture'] == 'vector':\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                NNinput = np.concatenate((NNinput ,np.arange(1,predHorNN+1)))\n",
    "            if normalizeData:\n",
    "                NNinput = normalize(NNinput.reshape(1,-1))\n",
    "            else:\n",
    "                NNinput = NNinput.reshape(1,-1)\n",
    "            tmpPred1h = np.array(NNmodel.predict(NNinput))\n",
    "        else:\n",
    "            raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                    Use either scalar or vector')\n",
    "\n",
    "        xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "        xp = np.arange(0.5,tmpPred1h.shape[1]+0.5,1/float(numUpSampleNN))\n",
    "        tmpPred = np.interp(xp,xp1h,tmpPred1h[0])\n",
    "        \n",
    "        # extend the last 1h prediction, as the NN produces predictions for 23 hours only (use regression)\n",
    "        #predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "        regNN = linear_model.LinearRegression()\n",
    "        histStepsRegrNN = numUpSampleNN\n",
    "        predStepsRegrNN = numUpSampleNN\n",
    "        regNN.fit(np.arange(0,histStepsRegrNN).reshape(-1, 1),np.array(tmpPred[-numUpSampleNN:]))\n",
    "        predRegrNN = regNN.predict(np.arange(histStepsRegrNN,histStepsRegrNN+predStepsRegrNN).reshape(-1, 1))\n",
    "        predRegrNN = np.maximum(np.zeros(len(predRegrNN)),predRegrNN)\n",
    "        predRegrNN.shape = (1,len(predRegrNN))\n",
    "        predNN = tmpPred.tolist() + predRegrNN.tolist()[0]\n",
    "        predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "        predNN = predNN*normalizeFactor\n",
    "        predNN.shape = (len(predNN),1)\n",
    "    else:\n",
    "        predNN = np.nan\n",
    "    \n",
    "    # Combine SARIMA and NN\n",
    "    if (models['alpha']['loaded']==True) & (models['sarima']['loaded']==True) & (models['nn']['loaded']==True):\n",
    "        alpha = models['alpha']['model']\n",
    "        alpha.shape = (len(alpha),1)\n",
    "        pred = np.multiply(alpha,predSARIMA) + np.multiply(1-alpha,predNN)\n",
    "    elif (models['sarima']['loaded']==True):\n",
    "        pred = predSARIMA\n",
    "    elif (models['nn']['loaded']==True):\n",
    "        pred = predNN\n",
    "    else:\n",
    "        raise ValueError('At least one forecast model must be loaded!')\n",
    "    \n",
    "    # Combine (SARIMA+NN) with regression model\n",
    "    if models['regression']['loaded']==True:\n",
    "        pred[0:predStepsRegr] = predRegr\n",
    "    \n",
    "    # Add logic to convert night values to zero\n",
    "    csrad = np.array(comb_df['clear_sky_forecast'])\n",
    "    csrad_upsampled = np.repeat(csrad,numUpSampleNN)\n",
    "    csrad_upsampled = np.append(csrad_upsampled,csrad_upsampled[-numUpSampleNN:])\n",
    "    indices = np.where(csrad_upsampled<1)\n",
    "    pred[indices] = 0\n",
    "    \n",
    "    # Resample from prediction time step to MPC time step\n",
    "    if resample2CtrlStep:\n",
    "        if timeStep>=timeStepCtrl:\n",
    "            if np.mod(timeStep,timeStepCtrl)==0:    \n",
    "                numUpSample = int(timeStep/timeStepCtrl)\n",
    "                predFinal = np.repeat(np.array(pred),numUpSample)\n",
    "            else:\n",
    "                raise ValueError('If prediction time step is larger than control time step, \\\n",
    "                prediction time step must be an integer multiple of control time step!')\n",
    "        else:\n",
    "            if np.mod(timeStepCtrl,timeStep)==0:    \n",
    "                numDownSample = int(timeStepCtrl/timeStep)\n",
    "                predFinal = np.mean(pred.reshape(-1,numDownSample),axis=1)\n",
    "            else:\n",
    "                raise ValueError('If control time step is larger than prediction time step, \\\n",
    "                control time step must be an integer multiple of prediction time step!')\n",
    "    else:\n",
    "        predFinal = pred\n",
    "    \n",
    "    predFinal.shape = (len(predFinal),1)\n",
    "    return predFinal, pred, predSARIMA, predNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPVforecast_v3(obsDf,wfDf,models,predHor=96,timeStep=15,timeStepCtrl=5,tsPeriod=96,retrainFlag=False,resample2CtrlStep=False):   \n",
    "    '''    \n",
    "    This function is called at each MPC execution to return the forecast of the uncertain disturbance\n",
    "    \n",
    "    Update: customized normalization for NN\n",
    "    \n",
    "    Inputs:\n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    models: a dictionary with the active models for forecasting\n",
    "        SARIMAorder: order of SARIMA model as (p,d,q,P,D,Q)\n",
    "        SARIMAparams: parameters of SARIMA model\n",
    "        NNmodel: object with the fitted Neural Network model\n",
    "        alpha: weighting factors to combine SARIMA and NN predictions. Final prediction=alpha*SARIMA+(1-alpha)*NN\n",
    "        normPowerCoeff: coefficient to normalize PV power data\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    timeStepCtrl: MPC control time step (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    wfDf: dataframe with necessary weather forecasts. Dataframe has only 1 row (latest forecast)\n",
    "    resample2CtrlStep: if True, the forecast will be resampled to the controller time step\n",
    "    \n",
    "    Outputs:\n",
    "    predFinal: forecasted uncertain variable (sampled in controller time step)\n",
    "    pred: forecasted uncertain variable (sampled in the time step of forecasting module) \n",
    "    predSARIMA: SARIMA forecast (sampled in the time step of forecasting module)\n",
    "    predNN: NN forecast (sampled in the time step of forecasting module)\n",
    "    '''\n",
    "    \n",
    "    # Get predictions from linear regression\n",
    "    if models['regression']['loaded']==True:\n",
    "        reg = linear_model.LinearRegression()\n",
    "        histStepsRegr = models['regression']['history']\n",
    "        predStepsRegr = models['regression']['prediction']\n",
    "        reg.fit(np.arange(0,histStepsRegr).reshape(-1, 1),np.array(obsDf[-histStepsRegr:]))\n",
    "        predRegr = reg.predict(np.arange(histStepsRegr,histStepsRegr+predStepsRegr).reshape(-1, 1))\n",
    "        predRegr = np.maximum(np.zeros(len(predRegr)),predRegr)\n",
    "        predRegr.shape = (len(predRegr),1)\n",
    "    else:\n",
    "        predRegr = np.nan\n",
    "    \n",
    "    # Get SARIMA predictions\n",
    "    if models['sarima']['loaded']==True:\n",
    "        obsDfSARIMA = obsDf.copy()\n",
    "        SARIMAorder = models['sarima']['model']['SARIMAorder']\n",
    "        SARIMAparams = models['sarima']['model']['SARIMAparams']\n",
    "        normalizeFactor = float(models['sarima']['normPowerCoeff'])\n",
    "        obsDfSARIMA = obsDfSARIMA/normalizeFactor\n",
    "        maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "        maxLag = np.amax([1, maxSeasLag])\n",
    "        \n",
    "        SARIMAmodel = SARIMAX(obsDfSARIMA[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                           seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "        SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)    \n",
    "        \n",
    "        if retrainFlag:\n",
    "            predSARIMA = multiStepSARIMAforecast_withRetrain(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor, timeStep, retrainFlag)\n",
    "        else:\n",
    "            #predSARIMA = multiStepSARIMAforecast(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "            yhat = SARIMAmodelFit.forecast(steps=predHor)\n",
    "            predSARIMA = yhat.values\n",
    "        \n",
    "        predSARIMA = np.maximum(np.zeros(len(predSARIMA)),predSARIMA)\n",
    "        predSARIMA = predSARIMA*normalizeFactor\n",
    "        predSARIMA.shape = (len(predSARIMA),1)\n",
    "    else:\n",
    "        predSARIMA = np.nan\n",
    "    \n",
    "    # Get NN predictions\n",
    "    if (models['nn']['loaded']==True):\n",
    "        obsDfNN = obsDf.copy()\n",
    "        NNmodel = models['nn']['model']\n",
    "        normalizeData = models['nn']['normInputData']\n",
    "        normalizeFactOutput = models['nn']['normPowerCoeff']\n",
    "        numUpSampleNN = int(60/timeStep)\n",
    "        predHorNN = 23\n",
    "        obsDfNN = obsDfNN/normalizeFactOutput\n",
    "        \n",
    "        # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "        if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "            cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df = comb_df[cols_df]\n",
    "            NNinput = np.concatenate(comb_df)\n",
    "        elif (models['nn']['inputData']==[True,True,True,False,False]) | (models['nn']['inputData']==[True,True,True,False,True]):\n",
    "            cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df = comb_df[cols_df]\n",
    "            NNinput = np.concatenate(comb_df)\n",
    "        elif (models['nn']['inputData']==[True,True,True,True,False]) | (models['nn']['inputData']==[True,True,True,True,True]):\n",
    "            cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "            obsDfdminus1 = obsDfNN.loc[obsDfNN.index>obsDfNN.index[-1]-pd.Timedelta('1 days 00:00:00')]\n",
    "            obsDfdminus1 = obsDfdminus1.resample('60T').mean()\n",
    "            obsDfdminus1 = obsDfdminus1.tolist()\n",
    "            obsDfdminus1 = obsDfdminus1[0:predHorNN]\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df['Observations_dminus1'] = obsDfdminus1\n",
    "            NNinput = np.concatenate(comb_df[cols_df])\n",
    "            \n",
    "        if models['nn']['architecture'] == 'scalar':\n",
    "            ncolsX = len(cols_df)\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "            Xf = np.empty([predHorNN,ncolsX])\n",
    "            Yf = np.empty([predHorNN,1])\n",
    "            idx = 0\n",
    "            for j in range(predHorNN):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(comb_df[k][j])]\n",
    "                if models['nn']['inputData'][-1]==True:\n",
    "                    tmp = tmp + [np.array(j+1)]\n",
    "                Xf[idx,:] = tmp\n",
    "                if normalizeData: \n",
    "                    Xf[idx,:] = normalizeInputNN(models,Xf[idx,:],cols_df,predHorNN)\n",
    "                Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                idx += 1\n",
    "            tmpPred1h = np.array(Yf)\n",
    "            tmpPred1h.shape=(1,len(tmpPred1h))\n",
    "        elif models['nn']['architecture'] == 'vector':\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                NNinput = np.concatenate((NNinput ,np.arange(1,predHorNN+1)))\n",
    "            if normalizeData:\n",
    "                NNinput = normalizeInputNN(models,NNinput,cols_df,predHorNN)\n",
    "            else:\n",
    "                NNinput = NNinput.reshape(1,-1)\n",
    "            tmpPred1h = np.array(NNmodel.predict(NNinput))\n",
    "        else:\n",
    "            raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                    Use either scalar or vector')\n",
    "\n",
    "        xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "        xp = np.arange(0.5,tmpPred1h.shape[1]+0.5,1/float(numUpSampleNN))\n",
    "        tmpPred = np.interp(xp,xp1h,tmpPred1h[0])\n",
    "        \n",
    "        # extend the last 1h prediction, as the NN produces predictions for 23 hours only (use regression)\n",
    "        #predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "        regNN = linear_model.LinearRegression()\n",
    "        histStepsRegrNN = numUpSampleNN\n",
    "        predStepsRegrNN = numUpSampleNN\n",
    "        regNN.fit(np.arange(0,histStepsRegrNN).reshape(-1, 1),np.array(tmpPred[-numUpSampleNN:]))\n",
    "        predRegrNN = regNN.predict(np.arange(histStepsRegrNN,histStepsRegrNN+predStepsRegrNN).reshape(-1, 1))\n",
    "        predRegrNN = np.maximum(np.zeros(len(predRegrNN)),predRegrNN)\n",
    "        predRegrNN.shape = (1,len(predRegrNN))\n",
    "        predNN = tmpPred.tolist() + predRegrNN.tolist()[0]\n",
    "        predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "        predNN = predNN*normalizeFactOutput\n",
    "        predNN.shape = (len(predNN),1)\n",
    "    else:\n",
    "        predNN = np.nan\n",
    "    \n",
    "    # Combine SARIMA and NN\n",
    "    if (models['alpha']['loaded']==True) & (models['sarima']['loaded']==True) & (models['nn']['loaded']==True):\n",
    "        alpha = models['alpha']['model']\n",
    "        alpha.shape = (len(alpha),1)\n",
    "        pred = np.multiply(alpha,predSARIMA) + np.multiply(1-alpha,predNN)\n",
    "    elif (models['sarima']['loaded']==True):\n",
    "        pred = predSARIMA\n",
    "    elif (models['nn']['loaded']==True):\n",
    "        pred = predNN\n",
    "    else:\n",
    "        raise ValueError('At least one forecast model must be loaded!')\n",
    "    \n",
    "    # Combine (SARIMA+NN) with regression model\n",
    "    if models['regression']['loaded']==True:\n",
    "        pred[0:predStepsRegr] = predRegr\n",
    "    \n",
    "    # Add logic to convert night values to zero\n",
    "    #csrad = np.array(wfDf['clear_sky_forecast'])\n",
    "    #csrad_upsampled = np.repeat(csrad,numUpSampleNN)\n",
    "    #csrad_upsampled = np.append(csrad_upsampled,csrad_upsampled[-numUpSampleNN:])\n",
    "    #indices = np.where(csrad_upsampled<1)   \n",
    "    obsDftmp = obsDf.copy()\n",
    "    obsDfdminus1tmp = obsDftmp.loc[obsDftmp.index>obsDftmp.index[-1]-pd.Timedelta('1 days 00:00:00')]\n",
    "    indices = np.where(np.array(obsDfdminus1tmp)<10)\n",
    "    pred[indices] = 0\n",
    "    \n",
    "    # Resample from prediction time step to MPC time step\n",
    "    if resample2CtrlStep:\n",
    "        if timeStep>=timeStepCtrl:\n",
    "            if np.mod(timeStep,timeStepCtrl)==0:    \n",
    "                numUpSample = int(timeStep/timeStepCtrl)\n",
    "                predFinal = np.repeat(np.array(pred),numUpSample)\n",
    "            else:\n",
    "                raise ValueError('If prediction time step is larger than control time step, \\\n",
    "                prediction time step must be an integer multiple of control time step!')\n",
    "        else:\n",
    "            if np.mod(timeStepCtrl,timeStep)==0:    \n",
    "                numDownSample = int(timeStepCtrl/timeStep)\n",
    "                predFinal = np.mean(pred.reshape(-1,numDownSample),axis=1)\n",
    "            else:\n",
    "                raise ValueError('If control time step is larger than prediction time step, \\\n",
    "                control time step must be an integer multiple of prediction time step!')\n",
    "    else:\n",
    "        predFinal = pred\n",
    "    \n",
    "    predFinal.shape = (len(predFinal),1)\n",
    "    return predFinal, pred, predSARIMA, predNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPVforecast_v4(obsDf,wfDf,models,predHor=96,timeStep=15,timeStepCtrl=5,tsPeriod=96,retrainFlag=False,resample2CtrlStep=False):   \n",
    "    '''    \n",
    "    This function is called at each MPC execution to return the forecast of the uncertain disturbance\n",
    "    \n",
    "    Update: customized normalization for NN\n",
    "    \n",
    "    Inputs:\n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    models: a dictionary with the active models for forecasting\n",
    "        SARIMAorder: order of SARIMA model as (p,d,q,P,D,Q)\n",
    "        SARIMAparams: parameters of SARIMA model\n",
    "        NNmodel: object with the fitted Neural Network model\n",
    "        alpha: weighting factors to combine SARIMA and NN predictions. Final prediction=alpha*SARIMA+(1-alpha)*NN\n",
    "        normPowerCoeff: coefficient to normalize PV power data\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    timeStepCtrl: MPC control time step (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    wfDf: dataframe with necessary weather forecasts. Dataframe has only 1 row (latest forecast)\n",
    "    resample2CtrlStep: if True, the forecast will be resampled to the controller time step\n",
    "    \n",
    "    Outputs:\n",
    "    predFinal: forecasted uncertain variable (sampled in controller time step)\n",
    "    pred: forecasted uncertain variable (sampled in the time step of forecasting module) \n",
    "    predSARIMA: SARIMA forecast (sampled in the time step of forecasting module)\n",
    "    predNN: NN forecast (sampled in the time step of forecasting module)\n",
    "    '''\n",
    "    \n",
    "    # Get predictions from linear regression\n",
    "    if models['regression']['loaded']==True:\n",
    "        reg = linear_model.LinearRegression()\n",
    "        histStepsRegr = models['regression']['history']\n",
    "        predStepsRegr = models['regression']['prediction']\n",
    "        reg.fit(np.arange(0,histStepsRegr).reshape(-1, 1),np.array(obsDf[-histStepsRegr:]))\n",
    "        predRegr = reg.predict(np.arange(histStepsRegr,histStepsRegr+predStepsRegr).reshape(-1, 1))\n",
    "        predRegr = np.maximum(np.zeros(len(predRegr)),predRegr)\n",
    "        predRegr.shape = (len(predRegr),1)\n",
    "    else:\n",
    "        predRegr = np.nan\n",
    "    \n",
    "    # Get SARIMA predictions\n",
    "    if models['sarima']['loaded']==True:\n",
    "        obsDfSARIMA = obsDf.copy()\n",
    "        SARIMAorder = models['sarima']['model']['SARIMAorder']\n",
    "        SARIMAparams = models['sarima']['model']['SARIMAparams']\n",
    "        normalizeFactor = float(models['sarima']['normPowerCoeff'])\n",
    "        obsDfSARIMA = obsDfSARIMA/normalizeFactor\n",
    "        maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "        maxLag = np.amax([1, maxSeasLag])\n",
    "        SARIMAmodel = SARIMAX(obsDfSARIMA[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                           seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "        SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)    \n",
    "        if retrainFlag:\n",
    "            predSARIMA = multiStepSARIMAforecast_withRetrain(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor, timeStep, retrainFlag)\n",
    "        else:\n",
    "            predSARIMA = multiStepSARIMAforecast(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "        predSARIMA = np.maximum(np.zeros(len(predSARIMA)),predSARIMA)\n",
    "        predSARIMA = predSARIMA*normalizeFactor\n",
    "        predSARIMA.shape = (len(predSARIMA),1)\n",
    "    else:\n",
    "        predSARIMA = np.nan\n",
    "    \n",
    "    # Get NN predictions\n",
    "    numUpSampleNN = int(60/timeStep)\n",
    "    if ((models['nn']['loaded']==True) & (wfDf['valid'].all()==True)):\n",
    "        obsDfNN = obsDf.copy()\n",
    "        NNmodel = models['nn']['model']\n",
    "        normalizeData = models['nn']['normInputData']\n",
    "        normalizeFactOutput = models['nn']['normPowerCoeff']\n",
    "        predHorNN = 23\n",
    "        obsDfNN = obsDfNN/normalizeFactOutput\n",
    "        \n",
    "        # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "        obsDfdminus1 = obsDfNN.loc[obsDfNN.index>obsDfNN.index[-1]-pd.Timedelta('1 days 00:00:00')]\n",
    "        minute_df = int(obsDfdminus1.index[-1].minute)\n",
    "        if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "            cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df = comb_df[cols_df]\n",
    "            NNinput = np.concatenate(comb_df)\n",
    "        elif (models['nn']['inputData']==[True,True,True,False,False]) | (models['nn']['inputData']==[True,True,True,False,True]):\n",
    "            cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df = comb_df[cols_df]\n",
    "            NNinput = np.concatenate(comb_df)\n",
    "        elif (models['nn']['inputData']==[True,True,True,True,False]) | (models['nn']['inputData']==[True,True,True,True,True]):\n",
    "            cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "            obsDfdminus1 = obsDfdminus1.resample('60T',base=minute_df).mean()\n",
    "            obsDfdminus1 = obsDfdminus1.tolist()\n",
    "            obsDfdminus1 = obsDfdminus1[0:predHorNN]\n",
    "            comb_df = wfDf.copy()\n",
    "            comb_df['Observations_dminus1'] = obsDfdminus1\n",
    "            NNinput = np.concatenate(comb_df[cols_df])\n",
    "            \n",
    "        if models['nn']['architecture'] == 'scalar':\n",
    "            ncolsX = len(cols_df)\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "            Xf = np.empty([predHorNN,ncolsX])\n",
    "            Yf = np.empty([predHorNN,1])\n",
    "            idx = 0\n",
    "            for j in range(predHorNN):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(comb_df[k][j])]\n",
    "                if models['nn']['inputData'][-1]==True:\n",
    "                    tmp = tmp + [np.array(j+1)]\n",
    "                Xf[idx,:] = tmp\n",
    "                if normalizeData: \n",
    "                    Xf[idx,:] = normalizeInputNN(models,Xf[idx,:],cols_df,predHorNN)\n",
    "                Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                idx += 1\n",
    "            tmpPred1h = np.array(Yf)\n",
    "            tmpPred1h.shape=(1,len(tmpPred1h))\n",
    "        elif models['nn']['architecture'] == 'vector':\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                NNinput = np.concatenate((NNinput ,np.arange(1,predHorNN+1)))\n",
    "            if normalizeData:\n",
    "                NNinput = normalizeInputNN(models,NNinput,cols_df,predHorNN)\n",
    "            else:\n",
    "                NNinput = NNinput.reshape(1,-1)\n",
    "            tmpPred1h = np.array(NNmodel.predict(NNinput))\n",
    "        else:\n",
    "            raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                    Use either scalar or vector')\n",
    "        \n",
    "        xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "        xp = np.arange(1,tmpPred1h.shape[1]+1,1/float(numUpSampleNN))\n",
    "        tmpPred = np.interp(xp,xp1h,tmpPred1h[0])\n",
    "        \n",
    "        # extend the last 1h prediction, as the NN produces predictions for 23 hours only (use regression)\n",
    "        #predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "        regNN = linear_model.LinearRegression()\n",
    "        histStepsRegrNN = numUpSampleNN\n",
    "        predStepsRegrNN = numUpSampleNN\n",
    "        regNN.fit(np.arange(0,histStepsRegrNN).reshape(-1, 1),np.array(tmpPred[-numUpSampleNN:]))\n",
    "        predRegrNN = regNN.predict(np.arange(histStepsRegrNN,histStepsRegrNN+predStepsRegrNN).reshape(-1, 1))\n",
    "        predRegrNN = np.maximum(np.zeros(len(predRegrNN)),predRegrNN)\n",
    "        predRegrNN.shape = (1,len(predRegrNN))\n",
    "        predNN = tmpPred.tolist() + predRegrNN.tolist()[0]\n",
    "        predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "        predNN = predNN*normalizeFactOutput\n",
    "        predNN.shape = (len(predNN),1)\n",
    "    else:\n",
    "        predNN = np.array([-1]*(predHor-4))\n",
    "    \n",
    "    # Combine SARIMA and NN\n",
    "    if (models['alpha']['loaded']==True) & (models['sarima']['loaded']==True) & ((models['nn']['loaded']==True) & (wfDf['valid'].all()==True)):\n",
    "        alpha = models['alpha']['model']\n",
    "        alpha.shape = (len(alpha),1)\n",
    "        pred = np.multiply(alpha,predSARIMA) + np.multiply(1-alpha,predNN)\n",
    "    elif (models['sarima']['loaded']==True):\n",
    "        pred = predSARIMA\n",
    "    elif ((models['nn']['loaded']==True) & (wfDf['valid'].all()==True)):\n",
    "        pred = predNN\n",
    "    else:\n",
    "        raise ValueError('At least one forecast model must be loaded!')\n",
    "    \n",
    "    # Combine (SARIMA+NN) with regression model\n",
    "    if models['regression']['loaded']==True:\n",
    "        pred[0:predStepsRegr] = predRegr\n",
    "    \n",
    "    # Add logic to convert night values to zero\n",
    "    csrad = np.array(wfDf['clear_sky_forecast'])\n",
    "    csrad_upsampled = np.repeat(csrad,numUpSampleNN)\n",
    "    csrad_upsampled = np.append(csrad_upsampled,csrad_upsampled[-numUpSampleNN:])\n",
    "    indices = np.where(csrad_upsampled<1)\n",
    "    pred[indices] = 0\n",
    "    \n",
    "    # Remove the 24th hour prediction\n",
    "    pred = pred[0:predHor-4]\n",
    "    if models['sarima']['loaded']==True: predSARIMA = predSARIMA[0:predHor-4]\n",
    "    if ((models['nn']['loaded']==True) & (wfDf['valid'].all()==True)): predNN = predNN[0:predHor-4]\n",
    "    \n",
    "    # Resample from prediction time step to MPC time step\n",
    "    if resample2CtrlStep:\n",
    "        if timeStep>=timeStepCtrl:\n",
    "            if np.mod(timeStep,timeStepCtrl)==0:    \n",
    "                numUpSample = int(timeStep/timeStepCtrl)\n",
    "                predFinal = np.repeat(np.array(pred),numUpSample)\n",
    "            else:\n",
    "                raise ValueError('If prediction time step is larger than control time step, \\\n",
    "                prediction time step must be an integer multiple of control time step!')\n",
    "        else:\n",
    "            if np.mod(timeStepCtrl,timeStep)==0:    \n",
    "                numDownSample = int(timeStepCtrl/timeStep)\n",
    "                predFinal = np.mean(pred.reshape(-1,numDownSample),axis=1)\n",
    "            else:\n",
    "                raise ValueError('If control time step is larger than prediction time step, \\\n",
    "                control time step must be an integer multiple of prediction time step!')\n",
    "    else:\n",
    "        predFinal = pred\n",
    "    \n",
    "    predFinal.shape = (len(predFinal),1)\n",
    "    return predFinal, pred, predSARIMA, predNN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainSARIMAmodel(obsDf,filename1,filename2, tsPeriod=96):   \n",
    "    '''\n",
    "    This function retrains the params of SARIMA model.\n",
    "    The model structure (p,d,q,P,D,Q orders) is fixed to the previously trained model.\n",
    "    \n",
    "    Inputs:   \n",
    "    obsDf: up-to-date dataframe series of total PV power (possibly normalized with rated PV power). Dataframe must have a frequency equal to timeStep.\n",
    "    filename1: file with current trained model\n",
    "    filename2: file to write the newly trained model\n",
    "    \n",
    "    Outputs:\n",
    "    SARIMA_model_updated (SARIMAorder, SARIMAparams): newly trained model\n",
    "    '''\n",
    "\n",
    "    with open(filename1, 'rb') as f:\n",
    "        SARIMAres = json.load(f)\n",
    "    SARIMAorder = np.array(SARIMAres['order']) # fix the already identified optimal order structure\n",
    "    SARIMA_params_old = np.array(SARIMAres['params'])\n",
    "\n",
    "    train = obsDf.copy()\n",
    "    model = SARIMAX(train, order=(SARIMAorder[0], SARIMAorder[1], SARIMAorder[2]), \n",
    "                    seasonal_order=(SARIMAorder[3], SARIMAorder[4], SARIMAorder[5], tsPeriod))\n",
    "    model_fit = model.fit(disp=0, method='lbfgs',start_params=SARIMA_params_old)\n",
    "    SARIMAparams = model_fit.params\n",
    "\n",
    "    SARIMA_model_updated = {'order':SARIMAorder.tolist(), 'params':SARIMAparams.get_values().tolist()}\n",
    "    with open(filename2, 'wb') as f:\n",
    "        json.dump(SARIMA_model_updated, f)\n",
    "        \n",
    "    return SARIMAorder, SARIMAparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeCombinedDf_NN_v1(wf_df,obsDf):\n",
    "    '''\n",
    "    This function puts together the weather data and historical power data \n",
    "    in the format that the NN training function expects.\n",
    "    \n",
    "    Inputs:\n",
    "    wf_df: historical weather forecast dataframe\n",
    "    obsDf: historical power dataframe (dataframe has only one data column)\n",
    "    wf_df and obsDf have both the same size and frequency of 1 hour\n",
    "    \n",
    "    Outputs:\n",
    "    comb_df: the combined dataframe\n",
    "    '''\n",
    "    predHor = int(len(wf_df['Tamb_forecast'].iloc[0])) # NN prediction horizon\n",
    "    \n",
    "    i = 0\n",
    "    dfs_overlap = True\n",
    "    while (obsDf.index[i]!=wf_df.index[0]): \n",
    "        i+=1\n",
    "        if i>len(obsDf):\n",
    "            dfs_overlap = False\n",
    "    if (dfs_overlap==False) | (len(obsDf[i:])<predHor):\n",
    "        raise ValueError('The weather forecast and power measurements dataframes do not overlap!')\n",
    "    \n",
    "    numPoints = np.amin([len(wf_df),len(obsDf.iloc[i:-predHor])]) # number of input-output pairs for NN training\n",
    "    comb_df = wf_df.iloc[0:numPoints]\n",
    "    print(len(wf_df))\n",
    "    print(len(comb_df))\n",
    "    Parray = []\n",
    "    cnt = 0\n",
    "    for idx in range(i,len(obsDf)-predHor):\n",
    "        Parray = Parray + [obsDf.iloc[idx:idx+predHor].tolist()]\n",
    "        cnt+=1\n",
    "        if cnt>=len(wf_df): break\n",
    "    comb_df['Observations'] = Parray # power measurements\n",
    "    \n",
    "    return comb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeCombinedDf_NN(wf_df,obsDf):\n",
    "    '''\n",
    "    This function puts together the weather data and historical power data \n",
    "    in the format that the NN training function expects.\n",
    "    \n",
    "    Inputs:\n",
    "    wf_df: historical weather forecast dataframe\n",
    "    obsDf: historical power dataframe (dataframe has only one data column)\n",
    "    wf_df and obsDf have both the same size and frequency of 1 hour\n",
    "    \n",
    "    Outputs:\n",
    "    comb_df: the combined dataframe\n",
    "    '''\n",
    "    predHor = int(len(wf_df[wf_df.columns[0]].iloc[0])) # NN prediction horizon\n",
    "    intersect = np.intersect1d(obsDf.index, wf_df.index)\n",
    "    comb_df = wf_df.loc[intersect]\n",
    "    #tmpP = obsDf\n",
    "    tmpP = obsDf.loc[intersect]\n",
    "    Parray = []\n",
    "    Parray_dminus1 = []\n",
    "    dropIdx = []\n",
    "    for idx in range(len(comb_df.index)):\n",
    "        #if (sum([comb_df.index[idx]-pd.Timedelta('0 days '+str(hourIdx)+':00:00') in tmpP.index for hourIdx in range(1,predHor+1)])==predHor):\n",
    "        if (sum([comb_df.index[idx]+pd.Timedelta('0 days '+str(hourIdx)+':00:00') in tmpP.index for hourIdx in range(1,predHor+1)])==predHor) & (sum([comb_df.index[idx]-pd.Timedelta('0 days '+str(hourIdx)+':00:00') in tmpP.index for hourIdx in range(1,predHor+1)])==predHor):\n",
    "            Parray = Parray + [tmpP.loc[((tmpP.index>=comb_df.index[idx]+pd.Timedelta('0 days 01:00:00')) &\n",
    "                                              (tmpP.index<=comb_df.index[idx]+pd.Timedelta('0 days '+str(hourIdx)+':00:00')))].tolist()]\n",
    "            Parray_dminus1 = Parray_dminus1 + [tmpP.loc[((tmpP.index<=comb_df.index[idx]-pd.Timedelta('0 days 01:00:00')) &\n",
    "                                              (tmpP.index>=comb_df.index[idx]-pd.Timedelta('0 days '+str(hourIdx)+':00:00')))].tolist()]\n",
    "        else:\n",
    "            dropIdx.append(idx)\n",
    "    comb_df = comb_df.drop(comb_df.index[dropIdx])\n",
    "    comb_df['Observations'] = Parray\n",
    "    comb_df['Observations_dminus1'] = Parray_dminus1\n",
    "\n",
    "    return comb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainNNmodel(models, comb_df, hyperParams, normInputData, normOutputFact, filename, filenameAux, testSetPerc=0.8):\n",
    "    '''\n",
    "    This function retrains the params of the NN model. \n",
    "    \n",
    "    Inputs:\n",
    "    models: prediction models\n",
    "    comb_df: dataframe that combines weather data and historical observations (power data)\n",
    "    hyperParams: dictionary with the grid search for hyper parameters (the range of each hyper-parameter is given as a list).\n",
    "    normInputData: If True, the NN input data are normalized before training\n",
    "    normOutputFact: the factor with which the historical observations are scaled before training\n",
    "    testSetPerc: proportion (from 0 to 1) of comb_df that is used for validation/testing\n",
    "    filename: the filename to save the newly trained NN model\n",
    "    '''\n",
    "    \n",
    "    # Split observation dataframe to training and validation sets\n",
    "    predHor = int(len(comb_df[comb_df.columns[0]].iloc[0])) # NN prediction horizon\n",
    "    trainSetPerc = 1-testSetPerc\n",
    "    trainSize = int(trainSetPerc*len(comb_df))\n",
    "    trainSet = []\n",
    "    if (np.isnan(models['nn']['randomSeed'])==False): \n",
    "        np.random.seed(seed=models['nn']['randomSeed'])\n",
    "    while (len(trainSet)<trainSize):\n",
    "        newElem = int(np.random.uniform(0,len(comb_df),1))\n",
    "        if newElem not in trainSet:\n",
    "            trainSet = trainSet + [newElem]\n",
    "    trainSet = np.sort(trainSet)\n",
    "    trainNN_df = comb_df.iloc[trainSet]\n",
    "    testSet = np.setdiff1d(range(len(comb_df)),trainSet)\n",
    "    testNN_df = comb_df.iloc[testSet]\n",
    "    \n",
    "    # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "    if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "        cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    \n",
    "    # Select NN architecture\n",
    "    if models['nn']['architecture']=='scalar':\n",
    "        nrows = len(trainNN_df)*predHor\n",
    "        ncolsX = int(len(np.concatenate(trainNN_df[cols_df].iloc[0]))/float(predHor))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "        ncolsY = 1\n",
    "        X = np.empty([nrows,ncolsX])\n",
    "        Y = np.empty([nrows,ncolsY])\n",
    "        idx = 0\n",
    "        for i in range(len(trainNN_df)):\n",
    "            for j in range(predHor):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(trainNN_df[k].iloc[i][j])]\n",
    "                if models['nn']['inputData'][-1]==True: tmp = tmp + [np.array(j+1)]\n",
    "                X[idx,:] = tmp\n",
    "                Y[idx,:] = np.array(trainNN_df['Observations'].iloc[i][j])\n",
    "                idx += 1\n",
    "        Y = Y.ravel() \n",
    "        if normInputData:\n",
    "            X = normalize(X)      \n",
    "    elif models['nn']['architecture']=='vector':\n",
    "        # Prepare and normalize input/output data \n",
    "        nrows = len(trainNN_df)\n",
    "        ncolsX = len(np.concatenate(trainNN_df[cols_df].iloc[0]))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=predHor\n",
    "        ncolsY = predHor\n",
    "        X = np.empty([nrows,ncolsX])\n",
    "        for i in range(nrows):\n",
    "            tmp = np.concatenate(trainNN_df[cols_df].iloc[i])\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                tmp = np.concatenate((tmp ,np.arange(1,predHor+1)))\n",
    "            X[i,:] = tmp\n",
    "        Y = np.empty([nrows,ncolsY])\n",
    "        for i in range(nrows):\n",
    "            Y[i,:] = trainNN_df['Observations'].iloc[i]\n",
    "        if normInputData:\n",
    "            X = normalize(X)\n",
    "    else:\n",
    "        raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                Use either scalar or vector')\n",
    "    idx=0\n",
    "    numCases_lbfgs = len(hyperParams['hidden_layer_sizes'])*len(hyperParams['activation'])*len(hyperParams['max_iter'])*len(hyperParams['early_stopping'])*len(hyperParams['validation_fraction'])\n",
    "    numCases_adam = len(hyperParams['hidden_layer_sizes'])*len(hyperParams['activation'])*len(hyperParams['max_iter'])*len(hyperParams['early_stopping'])*len(hyperParams['validation_fraction'])\n",
    "    numCases_sgd = len(hyperParams['hidden_layer_sizes'])*len(hyperParams['activation'])*len(hyperParams['max_iter'])*len(hyperParams['early_stopping'])*len(hyperParams['validation_fraction'])*len(hyperParams['learning_rate'])\n",
    "    if 'lbfgs' in hyperParams['solver']: lbfgs=1\n",
    "    else: lbfgs=0\n",
    "    if 'sgd' in hyperParams['solver']: sgd=1\n",
    "    else: sgd=0\n",
    "    if 'adam' in hyperParams['solver']: adam=1\n",
    "    else: adam=0    \n",
    "    numCases_tot = lbfgs*numCases_lbfgs + adam*numCases_adam + sgd*numCases_sgd\n",
    "    NNmodel_list = []\n",
    "    RMSE_insample_list = []\n",
    "    for hidden_layer_sizes in hyperParams['hidden_layer_sizes']:\n",
    "        for activation in hyperParams['activation']:\n",
    "            for solver in hyperParams['solver']:\n",
    "                for learning_rate in hyperParams['learning_rate']:\n",
    "                    for max_iter in hyperParams['max_iter']:\n",
    "                        for early_stopping in hyperParams['early_stopping']:\n",
    "                            for validation_fraction in hyperParams['validation_fraction']:\n",
    "                                if ((solver=='sgd') | (((solver=='lbfgs') | (solver=='adam')) & (learning_rate=='constant'))):\n",
    "                                    NNmodel = MLPRegressor(solver=solver,hidden_layer_sizes=hidden_layer_sizes,activation=activation,max_iter=max_iter,\n",
    "                                                           validation_fraction=validation_fraction,learning_rate=learning_rate,early_stopping=early_stopping)\n",
    "                                    NNmodel.fit(X,Y)\n",
    "                                    NNmodel_list  = NNmodel_list + [NNmodel]\n",
    "                                    Yf_insample = NNmodel.predict(X)\n",
    "                                    RMSE_insample_list = RMSE_insample_list + [rmse(Y,Yf_insample)]\n",
    "                                    idx+=1\n",
    "                                    print('Percentage (%) finished for given architecture: {}'.format(100*np.round(idx/float(numCases_tot),3)))\n",
    "    RMSE_list = []\n",
    "    RMSE_day_list = [] # RMSE only for daytime\n",
    "    \n",
    "    for NNmodel in NNmodel_list:\n",
    "        # Forecast with NN (out-of-sample)\n",
    "        if models['nn']['architecture']=='scalar':\n",
    "            nrows = len(testNN_df)*predHor\n",
    "            ncolsX = int(len(np.concatenate(testNN_df[cols_df].iloc[0]))/float(predHor))\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "            ncolsY = 1\n",
    "            Xf = np.empty([nrows,ncolsX])\n",
    "            Yf = np.empty([nrows,ncolsY])\n",
    "            Yr = np.empty([nrows,ncolsY])\n",
    "            idx = 0\n",
    "            for i in range(len(testNN_df)):\n",
    "                for j in range(predHor):\n",
    "                    tmp = []\n",
    "                    for k in cols_df:\n",
    "                        tmp = tmp + [np.array(testNN_df[k].iloc[i][j])]\n",
    "                    if models['nn']['inputData'][-1]==True:\n",
    "                        tmp = tmp + [np.array(j+1)]\n",
    "                    Xf[idx,:] = tmp\n",
    "                    if normInputData: \n",
    "                        Xf[idx,:] = normalize(Xf[idx,:].reshape(1,-1))\n",
    "                    Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                    Yf[idx,:] = normOutputFact*Yf[idx,:] # de-normalize\n",
    "                    Yr[idx,:] = normOutputFact*testNN_df['Observations'].iloc[i][j]\n",
    "                    idx += 1\n",
    "        elif models['nn']['architecture']=='vector':\n",
    "            nrows = len(testNN_df)\n",
    "            ncolsX = len(np.concatenate(testNN_df[cols_df].iloc[0]))\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=predHor\n",
    "            ncolsY = len(testNN_df['Observations'].iloc[0])\n",
    "            Xf = np.empty([nrows,ncolsX])\n",
    "            Yf = np.empty([nrows,ncolsY])\n",
    "            Yr = np.empty([nrows,ncolsY])\n",
    "            for i in range(nrows):\n",
    "                tmp = np.concatenate(testNN_df[cols_df].iloc[i])\n",
    "                if models['nn']['inputData'][-1]==True:\n",
    "                    tmp = np.concatenate((tmp ,np.arange(1,predHor+1)))\n",
    "                Xf[i,:] = tmp\n",
    "                if normInputData:\n",
    "                    Xf[i,:] = normalize(Xf[i,:].reshape(1,-1))\n",
    "                Yf[i,:] = NNmodel.predict(Xf[i,:].reshape(1,-1)) # NN forecasts\n",
    "                Yf[i,:] = normOutputFact*Yf[i,:] # de-normalize\n",
    "                Yr[i,:] = np.array(normOutputFact)*testNN_df['Observations'].iloc[i]\n",
    "        else:\n",
    "            raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                    Use either scalar or vector')\n",
    "        RMSE_list = RMSE_list + [rmse(Yr,Yf)]\n",
    "        daytime_idx = np.where(Yr>10) # larger than 10 Watt\n",
    "        RMSE_day_list = RMSE_day_list + [rmse(Yr[daytime_idx],Yf[daytime_idx])]\n",
    "    \n",
    "    RMSE_list_sorted_index = np.argsort(RMSE_list)\n",
    "    bestRMSE_index = RMSE_list_sorted_index[0]\n",
    "    NNmodel_best = NNmodel_list[bestRMSE_index]\n",
    "    joblib.dump(NNmodel_best, filename)\n",
    "    \n",
    "    with open('RMSE_insample_list_'+filenameAux+'.json', 'wb') as f:\n",
    "        json.dump(RMSE_insample_list, f)\n",
    "    with open('RMSE_list_'+filenameAux+'.json', 'wb') as f:\n",
    "        json.dump(RMSE_list, f)\n",
    "    with open('RMSE_day_list_'+filenameAux+'.json', 'wb') as f:\n",
    "        json.dump(RMSE_day_list, f)\n",
    "    joblib.dump(NNmodel_list, 'NNmodel_list_'+filenameAux+'.sav')\n",
    "    \n",
    "    return NNmodel_best, RMSE_insample_list, RMSE_list, RMSE_day_list, NNmodel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainNNmodel_v2(models, comb_df, hyperParams, normInputData, normOutputFact, filename, filenameAux, testSetPerc=0.2):\n",
    "    '''\n",
    "    This function retrains the params of the NN model. \n",
    "    \n",
    "    Update: customized normalization for NN\n",
    "    \n",
    "    Inputs:\n",
    "    models: prediction models\n",
    "    comb_df: dataframe that combines weather data and historical observations (power data)\n",
    "    hyperParams: dictionary with the grid search for hyper parameters (the range of each hyper-parameter is given as a list).\n",
    "    normInputData: If True, the NN input data are normalized before training\n",
    "    normOutputFact: the factor with which the historical observations are scaled before training\n",
    "    testSetPerc: proportion (from 0 to 1) of comb_df that is used for validation/testing\n",
    "    filename: the filename to save the newly trained NN model\n",
    "    '''\n",
    "    \n",
    "    # Split observation dataframe to training and validation sets\n",
    "    predHor = int(len(comb_df[comb_df.columns[0]].iloc[0])) # NN prediction horizon\n",
    "    trainSetPerc = 1-testSetPerc\n",
    "    trainSize = int(trainSetPerc*len(comb_df))\n",
    "    trainSet = []\n",
    "    if (np.isnan(models['nn']['randomSeed'])==False): \n",
    "        np.random.seed(seed=models['nn']['randomSeed'])\n",
    "    while (len(trainSet)<trainSize):\n",
    "        newElem = int(np.random.uniform(0,len(comb_df),1))\n",
    "        if newElem not in trainSet:\n",
    "            trainSet = trainSet + [newElem]\n",
    "    trainSet = np.sort(trainSet)\n",
    "    trainNN_df = comb_df.iloc[trainSet]\n",
    "    testSet = np.setdiff1d(range(len(comb_df)),trainSet)\n",
    "    testNN_df = comb_df.iloc[testSet]\n",
    "    \n",
    "    # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "    if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "        cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    \n",
    "    # Select NN architecture\n",
    "    if models['nn']['architecture']=='scalar':\n",
    "        nrows = len(trainNN_df)*predHor\n",
    "        ncolsX = int(len(np.concatenate(trainNN_df[cols_df].iloc[0]))/float(predHor))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "        ncolsY = 1\n",
    "        X = np.empty([nrows,ncolsX])\n",
    "        Y = np.empty([nrows,ncolsY])\n",
    "        idx = 0\n",
    "        notnanIdx = []\n",
    "        for i in range(len(trainNN_df)):\n",
    "            for j in range(predHor):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(trainNN_df[k].iloc[i][j])]\n",
    "                if models['nn']['inputData'][-1]==True: tmp = tmp + [np.array(j+1)]\n",
    "                X[idx,:] = tmp\n",
    "                Y[idx,:] = np.array(trainNN_df['Observations'].iloc[i][j])\n",
    "                if (len(np.where(np.isnan(X[idx,:]))[0])==0) & (~np.isnan(Y[idx,:])):\n",
    "                    notnanIdx = notnanIdx+[idx]\n",
    "                idx += 1\n",
    "        X = X[notnanIdx,:]\n",
    "        Y = Y[notnanIdx,:]\n",
    "        Y = Y.ravel() \n",
    "        if normInputData:\n",
    "            X = normalizeInputNN(models,X,cols_df,predHor) \n",
    "    elif models['nn']['architecture']=='vector':\n",
    "        # Prepare and normalize input/output data \n",
    "        nrows = len(trainNN_df)\n",
    "        ncolsX = len(np.concatenate(trainNN_df[cols_df].iloc[0]))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=predHor\n",
    "        ncolsY = predHor\n",
    "        X = np.empty([nrows,ncolsX])\n",
    "        Y = np.empty([nrows,ncolsY])\n",
    "        notnanIdx = []\n",
    "        for i in range(nrows):\n",
    "            tmp = np.concatenate(trainNN_df[cols_df].iloc[i])\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                tmp = np.concatenate((tmp ,np.arange(1,predHor+1)))\n",
    "            X[i,:] = tmp\n",
    "            Y[i,:] = trainNN_df['Observations'].iloc[i]\n",
    "            if (len(np.where(np.isnan(X[i,:]))[0])==0) & (len(np.where(np.isnan(Y[i,:]))[0])==0):\n",
    "                notnanIdx = notnanIdx+[i]\n",
    "        X = X[notnanIdx,:]\n",
    "        Y = Y[notnanIdx,:]\n",
    "        if normInputData:\n",
    "            X = normalizeInputNN(models,X,cols_df,predHor) \n",
    "    else:\n",
    "        raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                Use either scalar or vector')\n",
    "    idx=0\n",
    "    numCases_lbfgs = len(hyperParams['hidden_layer_sizes'])*len(hyperParams['activation'])*len(hyperParams['max_iter'])*len(hyperParams['early_stopping'])*len(hyperParams['validation_fraction'])\n",
    "    numCases_adam = len(hyperParams['hidden_layer_sizes'])*len(hyperParams['activation'])*len(hyperParams['max_iter'])*len(hyperParams['early_stopping'])*len(hyperParams['validation_fraction'])\n",
    "    numCases_sgd = len(hyperParams['hidden_layer_sizes'])*len(hyperParams['activation'])*len(hyperParams['max_iter'])*len(hyperParams['early_stopping'])*len(hyperParams['validation_fraction'])*len(hyperParams['learning_rate'])\n",
    "    if 'lbfgs' in hyperParams['solver']: lbfgs=1\n",
    "    else: lbfgs=0\n",
    "    if 'sgd' in hyperParams['solver']: sgd=1\n",
    "    else: sgd=0\n",
    "    if 'adam' in hyperParams['solver']: adam=1\n",
    "    else: adam=0    \n",
    "    numCases_tot = lbfgs*numCases_lbfgs + adam*numCases_adam + sgd*numCases_sgd\n",
    "    NNmodel_list = []\n",
    "    RMSE_insample_list = []\n",
    "    for hidden_layer_sizes in hyperParams['hidden_layer_sizes']:\n",
    "        for activation in hyperParams['activation']:\n",
    "            for solver in hyperParams['solver']:\n",
    "                for learning_rate in hyperParams['learning_rate']:\n",
    "                    for max_iter in hyperParams['max_iter']:\n",
    "                        for early_stopping in hyperParams['early_stopping']:\n",
    "                            for validation_fraction in hyperParams['validation_fraction']:\n",
    "                                if ((solver=='sgd') | (((solver=='lbfgs') | (solver=='adam')) & (learning_rate=='constant'))):\n",
    "                                    NNmodel = MLPRegressor(solver=solver,hidden_layer_sizes=hidden_layer_sizes,activation=activation,max_iter=max_iter,\n",
    "                                                           validation_fraction=validation_fraction,learning_rate=learning_rate,early_stopping=early_stopping)\n",
    "                                    NNmodel.fit(X,Y)\n",
    "                                    NNmodel_list  = NNmodel_list + [NNmodel]\n",
    "                                    Yf_insample = NNmodel.predict(X)\n",
    "                                    RMSE_insample_list = RMSE_insample_list + [rmse(Y,Yf_insample)]\n",
    "                                    idx+=1\n",
    "                                    print('Percentage (%) finished for given architecture: {}'.format(100*np.round(idx/float(numCases_tot),3)))\n",
    "    RMSE_list = []\n",
    "    RMSE_day_list = [] # RMSE only for daytime\n",
    "    \n",
    "    for NNmodel in NNmodel_list:\n",
    "        # Forecast with NN (out-of-sample)\n",
    "        if models['nn']['architecture']=='scalar':\n",
    "            nrows = len(testNN_df)*predHor\n",
    "            ncolsX = int(len(np.concatenate(testNN_df[cols_df].iloc[0]))/float(predHor))\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "            ncolsY = 1\n",
    "            Xf = np.empty([nrows,ncolsX])\n",
    "            Yf = np.empty([nrows,ncolsY])\n",
    "            Yr = np.empty([nrows,ncolsY])\n",
    "            idx = 0\n",
    "            notnanIdx = []\n",
    "            for i in range(len(testNN_df)):\n",
    "                for j in range(predHor):\n",
    "                    tmp = []\n",
    "                    for k in cols_df:\n",
    "                        tmp = tmp + [np.array(testNN_df[k].iloc[i][j])]\n",
    "                    if models['nn']['inputData'][-1]==True:\n",
    "                        tmp = tmp + [np.array(j+1)]\n",
    "                    Xf[idx,:] = tmp\n",
    "                    if (len(np.where(np.isnan(Xf[idx,:]))[0])==0):\n",
    "                        notnanIdx = notnanIdx+[idx]\n",
    "                        if normInputData: \n",
    "                            Xf[idx,:] = normalizeInputNN(models,Xf[idx,:],cols_df,predHor)\n",
    "                        Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                        Yf[idx,:] = normOutputFact*Yf[idx,:] # de-normalize\n",
    "                        Yr[idx,:] = normOutputFact*testNN_df['Observations'].iloc[i][j]\n",
    "                    idx += 1\n",
    "            Xf = Xf[notnanIdx,:]\n",
    "            Yf = Yf[notnanIdx,:]\n",
    "            Yr = Yr[notnanIdx,:]\n",
    "        elif models['nn']['architecture']=='vector':\n",
    "            nrows = len(testNN_df)\n",
    "            ncolsX = len(np.concatenate(testNN_df[cols_df].iloc[0]))\n",
    "            if models['nn']['inputData'][-1]==True: ncolsX+=predHor\n",
    "            ncolsY = len(testNN_df['Observations'].iloc[0])\n",
    "            Xf = np.empty([nrows,ncolsX])\n",
    "            Yf = np.empty([nrows,ncolsY])\n",
    "            Yr = np.empty([nrows,ncolsY])\n",
    "            notnanIdx = []\n",
    "            for i in range(nrows):\n",
    "                tmp = np.concatenate(testNN_df[cols_df].iloc[i])\n",
    "                if models['nn']['inputData'][-1]==True:\n",
    "                    tmp = np.concatenate((tmp ,np.arange(1,predHor+1)))\n",
    "                Xf[i,:] = tmp\n",
    "                if (len(np.where(np.isnan(Xf[i,:]))[0])==0):\n",
    "                    notnanIdx = notnanIdx+[i]\n",
    "                    if normInputData:\n",
    "                        Xf[i,:] = normalizeInputNN(models,Xf[i,:],cols_df,predHor)\n",
    "                    Yf[i,:] = NNmodel.predict(Xf[i,:].reshape(1,-1)) # NN forecasts\n",
    "                    Yf[i,:] = normOutputFact*Yf[i,:] # de-normalize\n",
    "                    Yr[i,:] = np.array(normOutputFact)*testNN_df['Observations'].iloc[i]\n",
    "            Xf = Xf[notnanIdx,:]\n",
    "            Yf = Yf[notnanIdx,:]\n",
    "            Yr = Yr[notnanIdx,:]\n",
    "        else:\n",
    "            raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                    Use either scalar or vector')\n",
    "        RMSE_list = RMSE_list + [rmse(Yr,Yf)]\n",
    "        daytime_idx = np.where(Yr>10) # larger than 10 Watt\n",
    "        RMSE_day_list = RMSE_day_list + [rmse(Yr[daytime_idx],Yf[daytime_idx])]\n",
    "    \n",
    "    RMSE_list_sorted_index = np.argsort(RMSE_list)\n",
    "    bestRMSE_index = RMSE_list_sorted_index[0]\n",
    "    NNmodel_best = NNmodel_list[bestRMSE_index]\n",
    "    joblib.dump(NNmodel_best, filename)\n",
    "    \n",
    "    with open('RMSE_insample_list_'+filenameAux+'.json', 'wb') as f:\n",
    "        json.dump(RMSE_insample_list, f)\n",
    "    with open('RMSE_list_'+filenameAux+'.json', 'wb') as f:\n",
    "        json.dump(RMSE_list, f)\n",
    "    with open('RMSE_day_list_'+filenameAux+'.json', 'wb') as f:\n",
    "        json.dump(RMSE_day_list, f)\n",
    "    joblib.dump(NNmodel_list, 'NNmodel_list_'+filenameAux+'.sav')\n",
    "    \n",
    "    return NNmodel_best, RMSE_insample_list, RMSE_list, RMSE_day_list, NNmodel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeWeights_v1(SARIMAorder,SARIMAparams,initSet,trainSet,predHor,timeStep,tsPeriod,\n",
    "                    NNmodel,normalizeData,normalizeFactorSARIMA,normalizeFactorNN,wf_df,filename):\n",
    "    \"\"\"\n",
    "    Use fitted SARIMA and NN models and identify the optimal weighting factors \n",
    "    to combine the two forecasts into one forecast.\n",
    "\n",
    "    SARIMAorder: list of orders of SARIMA model\n",
    "    SARIMAparams: list of params of SARIMA model\n",
    "    initSet: initialization set (used just to redefine the SARIMA structure with the \"filter\" method)\n",
    "    trainSet: training set\n",
    "    predHor: prediction horizon for the multiple-step ahead forecasts\n",
    "    tsPreTrainFlageriod: seasonality period (number of time steps for the SARIMA model)\n",
    "    NNmodel: trained NN model\n",
    "    normalizeData: if True, I/O data are normalized; otherwise, they are not\n",
    "    wf_df: weather data dataframe\n",
    "    filename: filename to save the newly computed weighting factors\n",
    "    \"\"\"\n",
    "    # Get individual predictions\n",
    "    simHor = len(trainSet) - predHor\n",
    "    obsMat = []\n",
    "    obsDf = initSet\n",
    "    predMatSARIMA = []\n",
    "    predMatNN = []\n",
    "    numUpSampleNN = int(60/timeStep)\n",
    "    maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "    maxLag = np.amax([1, maxSeasLag])\n",
    "    SARIMAmodel = SARIMAX(initSet[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                       seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "    SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)\n",
    "    \n",
    "    idx = 0\n",
    "    for t in range(simHor):\n",
    "        if (idx<len(wf_df)) & (trainSet.index[t].minute == 0) & (len(np.where(obsDf.isnull().iloc[-maxLag*predHor-1:])[0])==0) & (trainSet.isnull().iloc[t:t + predHor].any() == False):\n",
    "            # Find the next entry in wf_df that corresponds to the same hour as in trainSet\n",
    "            while (idx<len(wf_df)) & (wf_df.index[idx]!=trainSet.index[t]): \n",
    "                idx+=1\n",
    "            if (idx>=len(wf_df)): break\n",
    "            \n",
    "            # Get SARIMA predictions\n",
    "            curPred = multiStepSARIMAforecast(obsDf.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "            curPred = np.maximum(np.zeros(len(curPred)),curPred)\n",
    "            curPred = curPred*normalizeFactorSARIMA\n",
    "            predMatSARIMA = predMatSARIMA + [curPred.tolist()]\n",
    "                   \n",
    "            \n",
    "            # Get NN predictions\n",
    "            NNinput = np.concatenate(wf_df[['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']].iloc[idx])\n",
    "            if normalizeData: \n",
    "                NNinput = normalize(NNinput.reshape(1,-1))\n",
    "            else:\n",
    "                NNinput = NNinput.reshape(1,-1)\n",
    "            curPred1h = np.array(NNmodel.predict(NNinput))       \n",
    "            xp1h = np.arange(1,curPred1h.shape[1]+1)\n",
    "            xp = np.arange(1,curPred1h.shape[1]+1,1/float(numUpSampleNN))\n",
    "            curPred = np.interp(xp,xp1h,curPred1h[0])\n",
    "            idx+=1    \n",
    "            curPred = curPred.tolist() + curPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "            curPred = np.maximum(np.zeros(len(curPred)),curPred)\n",
    "            curPred = curPred*normalizeFactorNN\n",
    "            predMatNN = predMatNN + [curPred]\n",
    "            \n",
    "            # Collect observations\n",
    "            obsMat = obsMat + [trainSet.iloc[t:t + predHor].values]\n",
    "\n",
    "        # Update 'observation' dataframe\n",
    "        obs = pd.DataFrame(data=np.array([trainSet.iloc[t]]),index=[obsDf.index[-1] + pd.Timedelta(minutes=timeStep)])\n",
    "        obsDf = obsDf.append(obs)\n",
    "\n",
    "    predMatSARIMA = np.array(predMatSARIMA)\n",
    "    predMatNN = np.array(predMatNN)\n",
    "    obsMat = np.array(obsMat)\n",
    "    \n",
    "    predBlkSARIMA = np.diag(predMatSARIMA[0])\n",
    "    predBlkNN = np.diag(predMatNN[0])\n",
    "    numCols = predMatSARIMA.shape[0]\n",
    "\n",
    "    for i in range(1,numCols):\n",
    "        predBlkSARIMA = np.vstack((predBlkSARIMA,np.diag(predMatSARIMA[i])))\n",
    "        predBlkNN = np.vstack((predBlkNN,np.diag(predMatNN[i])))\n",
    "    \n",
    "    obsVec = np.reshape(obsMat, (obsMat.shape[0] * obsMat.shape[1],1))\n",
    "    \n",
    "    # Combine predictions and optimize weighting factor\n",
    "    alpha = cvp.Variable(predHor,1)\n",
    "    \n",
    "    residual = predBlkSARIMA*alpha + predBlkNN*(1-alpha) - obsVec\n",
    "    obj = cvp.norm(residual,2)\n",
    "    const = [alpha>=0, alpha<=1]\n",
    "    prob = cvp.Problem(cvp.Minimize(obj), const)\n",
    "    prob.solve(solver='ECOS')\n",
    "    weightFact = alpha.value\n",
    "    objValue = obj.value\n",
    "    \n",
    "    weightFactDict = {'alpha':weightFact.tolist()}\n",
    "    with open(filename, 'wb') as f:\n",
    "        json.dump(weightFactDict, f) \n",
    "    return weightFact\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeWeights_v2(models,SARIMAorder,SARIMAparams,initSet,trainSet,predHor,timeStep,tsPeriod,\n",
    "                    NNmodel,normalizeData,normalizeFactorSARIMA,normalizeFactorNN,wf_df,filename):\n",
    "    \"\"\"\n",
    "    Use fitted SARIMA and NN models and identify the optimal weighting factors \n",
    "    to combine the two forecasts into one forecast.\n",
    "\n",
    "    SARIMAorder: list of orders of SARIMA model\n",
    "    SARIMAparams: list of params of SARIMA model\n",
    "    initSet: initialization set (used just to redefine the SARIMA structure with the \"filter\" method)\n",
    "    trainSet: training set\n",
    "    predHor: prediction horizon for the multiple-step ahead forecasts\n",
    "    tsPreTrainFlageriod: seasonality period (number of time steps for the SARIMA model)\n",
    "    NNmodel: trained NN model\n",
    "    normalizeData: if True, I/O data are normalized; otherwise, they are not\n",
    "    wf_df: weather data dataframe\n",
    "    filename: filename to save the newly computed weighting factors\n",
    "    \"\"\"\n",
    "    # Get individual predictions\n",
    "    simHor = len(trainSet) - predHor\n",
    "    obsMat = []\n",
    "    obsDf = pd.Series(initSet) # obsDf is un-normalized\n",
    "    initSet = initSet/float(normalizeFactorSARIMA)\n",
    "    predMatSARIMA = []\n",
    "    predMatNN = []\n",
    "    numUpSampleNN = int(60/timeStep)\n",
    "    maxSeasLag = np.amax([SARIMAorder[2],SARIMAorder[5]])\n",
    "    maxLag = np.amax([1, maxSeasLag])\n",
    "    SARIMAmodel = SARIMAX(initSet[-maxLag*predHor-1:], order=(int(SARIMAorder[0]), int(SARIMAorder[1]), int(SARIMAorder[2])),\n",
    "                       seasonal_order=(int(SARIMAorder[3]), int(SARIMAorder[4]), int(SARIMAorder[5]), tsPeriod))\n",
    "    SARIMAmodelFit = SARIMAmodel.filter(SARIMAparams)\n",
    "    \n",
    "    idx = 0\n",
    "    predHorNN = 23\n",
    "    for t in range(simHor):\n",
    "        print('Iteration {} of {}'.format(t, simHor))\n",
    "        if (idx<len(wf_df)) & (trainSet.index[t].minute == 0) & (len(np.where(obsDf.isnull().iloc[-maxLag*predHor-1:])[0])==0) & (len(obsDf.loc[obsDf.index>obsDf.index[-1]-pd.Timedelta('1 days 00:00:00')])==96) & (trainSet.isnull().iloc[t:t + predHor].any() == False):\n",
    "            # Find the next entry in wf_df that corresponds to the same hour as in trainSet\n",
    "            while (idx<len(wf_df)) & (wf_df.index[idx]!=trainSet.index[t]): \n",
    "                idx+=1\n",
    "            if (idx>=len(wf_df)): break\n",
    "            \n",
    "            # Get SARIMA predictions\n",
    "            obsDfSARIMA = obsDf.copy()/float(normalizeFactorSARIMA)\n",
    "            curPred = multiStepSARIMAforecast(obsDfSARIMA.iloc[-maxLag*predHor-1:], SARIMAmodelFit, predHor)\n",
    "            curPred = np.maximum(np.zeros(len(curPred)),curPred)\n",
    "            curPred = curPred*normalizeFactorSARIMA\n",
    "            predMatSARIMA = predMatSARIMA + [curPred.tolist()]\n",
    "                   \n",
    "            # Get NN predictions\n",
    "            obsDfNN = obsDf.copy()/float(normalizeFactorNN)\n",
    "            numUpSampleNN = int(60/timeStep)\n",
    "\n",
    "            # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "            if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "                cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "                NNinput = np.concatenate(wf_df[cols_df].iloc[idx])\n",
    "            elif (models['nn']['inputData']==[True,True,True,False,False]) | (models['nn']['inputData']==[True,True,True,False,True]):\n",
    "                cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "                NNinput = np.concatenate(wf_df[cols_df].iloc[idx])\n",
    "            elif (models['nn']['inputData']==[True,True,True,True,False]) | (models['nn']['inputData']==[True,True,True,True,True]):\n",
    "                cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "                obsDfdminus1 = obsDfNN.loc[obsDfNN.index>obsDfNN.index[-1]-pd.Timedelta('1 days 00:00:00')]\n",
    "                minute_df = int(obsDfdminus1.index[-1].minute)\n",
    "                obsDfdminus1 = obsDfdminus1.resample('60T',base=minute_df).mean()\n",
    "                obsDfdminus1 = obsDfdminus1.tolist()\n",
    "                obsDfdminus1 = obsDfdminus1[0:predHorNN]\n",
    "                comb_df = wf_df.iloc[idx]\n",
    "                comb_df['Observations_dminus1'] = obsDfdminus1\n",
    "                NNinput = np.concatenate(comb_df[cols_df])\n",
    "\n",
    "            if models['nn']['architecture'] == 'scalar':\n",
    "                ncolsX = len(cols_df)\n",
    "                if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "                Xf = np.empty([predHorNN,ncolsX])\n",
    "                Yf = np.empty([predHorNN,1])\n",
    "                idx2 = 0\n",
    "                for j in range(predHorNN):\n",
    "                    tmp = []\n",
    "                    for k in cols_df:\n",
    "                        tmp = tmp + [np.array(comb_df[k][j])]\n",
    "                    if models['nn']['inputData'][-1]==True:\n",
    "                        tmp = tmp + [np.array(j+1)]\n",
    "                    Xf[idx2,:] = tmp\n",
    "                    if normalizeData: \n",
    "                        Xf[idx2,:] = normalizeInputNN(models,Xf[idx2,:],cols_df,predHorNN)\n",
    "                    Yf[idx2,:] = NNmodel.predict(Xf[idx2,:].reshape(1,-1)) # NN forecasts\n",
    "                    idx2 += 1\n",
    "                tmpPred1h = np.array(Yf)\n",
    "                tmpPred1h.shape=(1,len(tmpPred1h))\n",
    "            elif models['nn']['architecture'] == 'vector':\n",
    "                if models['nn']['inputData'][-1]==True:\n",
    "                    NNinput = np.concatenate((NNinput ,np.arange(1,predHorNN+1)))\n",
    "                if normalizeData:\n",
    "                    NNinput = normalizeInputNN(models,NNinput,cols_df,predHorNN)\n",
    "                tmpPred1h = np.array(NNmodel.predict(NNinput.reshape(1,-1)))\n",
    "            else:\n",
    "                raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                        Use either scalar or vector')\n",
    "\n",
    "            xp1h = np.arange(1,tmpPred1h.shape[1]+1)\n",
    "            xp = np.arange(1,tmpPred1h.shape[1]+1,1/float(numUpSampleNN))\n",
    "            tmpPred = np.interp(xp,xp1h,tmpPred1h[0])\n",
    "\n",
    "            # extend the last 1h prediction, as the NN produces predictions for 23 hours only (use regression)\n",
    "            #predNN = tmpPred.tolist() + tmpPred[-numUpSampleNN:].tolist() # extend the last 1h prediction, as the NN produces predictions for 23 hours only\n",
    "            regNN = linear_model.LinearRegression()\n",
    "            histStepsRegrNN = numUpSampleNN\n",
    "            predStepsRegrNN = numUpSampleNN\n",
    "            regNN.fit(np.arange(0,histStepsRegrNN).reshape(-1, 1),np.array(tmpPred[-numUpSampleNN:]))\n",
    "            predRegrNN = regNN.predict(np.arange(histStepsRegrNN,histStepsRegrNN+predStepsRegrNN).reshape(-1, 1))\n",
    "            predRegrNN = np.maximum(np.zeros(len(predRegrNN)),predRegrNN)\n",
    "            predRegrNN.shape = (1,len(predRegrNN))\n",
    "            predNN = tmpPred.tolist() + predRegrNN.tolist()[0]\n",
    "\n",
    "            predNN = np.maximum(np.zeros(len(predNN)),predNN)\n",
    "            predNN = predNN*normalizeFactorNN\n",
    "            predNN.shape = (len(predNN))\n",
    "            predMatNN = predMatNN + [predNN]\n",
    "            \n",
    "            # Collect observations\n",
    "            obsMat = obsMat + [trainSet.iloc[t:t + predHor].values]\n",
    "            idx+=1\n",
    "\n",
    "        # Update 'observation' dataframe\n",
    "        obs = pd.Series(data=np.array([trainSet.iloc[t]]),index=[obsDf.index[-1] + pd.Timedelta(minutes=timeStep)])\n",
    "        obsDf = obsDf.append(obs)\n",
    "\n",
    "    if idx>0:\n",
    "        predMatSARIMA = np.array(predMatSARIMA)\n",
    "        predMatNN = np.array(predMatNN)\n",
    "        obsMat = np.array(obsMat)\n",
    "\n",
    "        predBlkSARIMA = np.diag(predMatSARIMA[0])\n",
    "        predBlkNN = np.diag(predMatNN[0])\n",
    "        numCols = predMatSARIMA.shape[0]\n",
    "\n",
    "        for i in range(1,numCols):\n",
    "            predBlkSARIMA = np.vstack((predBlkSARIMA,np.diag(predMatSARIMA[i])))\n",
    "            predBlkNN = np.vstack((predBlkNN,np.diag(predMatNN[i])))\n",
    "\n",
    "        obsVec = np.reshape(obsMat, (obsMat.shape[0] * obsMat.shape[1],1))\n",
    "\n",
    "        # Combine predictions and optimize weighting factor\n",
    "        alpha = cvp.Variable(predHor,1)\n",
    "\n",
    "        residual = predBlkSARIMA*alpha + predBlkNN*(1-alpha) - obsVec\n",
    "        obj = cvp.norm(residual,2)\n",
    "        const = [alpha>=0, alpha<=1]\n",
    "        prob = cvp.Problem(cvp.Minimize(obj), const)\n",
    "        prob.solve(solver='ECOS')\n",
    "        weightFact = alpha.value\n",
    "        objValue = obj.value\n",
    "\n",
    "        weightFactDict = {'alpha':weightFact.tolist()}\n",
    "        with open(filename, 'wb') as f:\n",
    "            json.dump(weightFactDict, f) \n",
    "        return weightFact\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainCombinedModel(models,obsDf,wf_df,filenameSARIMAnew,filenameNNnew,filenameNNaux,filenameAlphaNew,testSetPercNN,\n",
    "                         hyperParams={'hidden_layer_sizes': [(2),(3),(4),(5),(6),(7),(8),(9),(10),(20),(30),(40),(50),(60),(70),(80),(90),(100),(110),(120),\n",
    "                                                            (2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(10,10),(20,20),(30,30),(40,40),(50,50),(60,60),(70,70),(80,80),(90,90),(100,100),(110,110),(120,120)],\n",
    "                                      'activation': ['identity','relu','logistic','tanh'],\n",
    "                                      'solver': ['lbfgs','sgd','adam'],'learning_rate': ['constant','invscaling','adaptive'],'max_iter': [10000],\n",
    "                                      'early_stopping': [True], 'validation_fraction': [0.1]},\n",
    "                         predHor=96,timeStep=15,tsPeriod=96):\n",
    "    '''\n",
    "    Wrapper function to re-train the combined prediction model\n",
    "    Step 1: re-train SARIMA\n",
    "    Step 2: re-train NN\n",
    "    Step 3: re-optimize weighting factors\n",
    "    \n",
    "    Inputs:\n",
    "    models: prediction models\n",
    "    obsDf: historical power dataframe (dataframe has only one data column)\n",
    "    wf_df: historical weather forecast dataframe\n",
    "    filenameSARIMAnew: filename to save newly trained SARIMA model\n",
    "    filenameNNnew: filename to save newly trained NN model\n",
    "    filenameAlphaNew: filename to save newly calculated alpha factors\n",
    "    testSetPercNN: proportion (from 0 to 1) of comb_df that is used for validation/testing\n",
    "    hyperParams: dictionary with the grid search for hyper parameters (the range of each hyper-parameter is given as a list).\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    '''\n",
    "    \n",
    "    # Retrain SARIMA\n",
    "    startTime = time.time()\n",
    "    filenameSARIMAold = models['sarima']['path']\n",
    "    normalizeFactorSARIMA = float(models['sarima']['normPowerCoeff'])\n",
    "    obsDf_SARIMA = obsDf.copy()/normalizeFactorSARIMA\n",
    "    SARIMAorder, SARIMAparams = retrainSARIMAmodel(obsDf_SARIMA,filenameSARIMAold,filenameSARIMAnew,tsPeriod)\n",
    "    print('SARIMA training: done in {} seconds'.format(np.round(time.time()-startTime)))\n",
    "    \n",
    "    # Retrain NN\n",
    "    startTime = time.time()\n",
    "    normalizeFactorNN = float(models['nn']['normPowerCoeff'])\n",
    "    normInputData = models['nn']['normInputData']\n",
    "    obsDf_NN = obsDf.copy()/normalizeFactorNN\n",
    "    obsDf_NN = obsDf_NN.resample('60T',base=obsDf_NN.index[0].minute).mean()\n",
    "    comb_df = makeCombinedDf_NN(wf_df,obsDf_NN)\n",
    "    NNmodel, RMSE_insample_list, RMSE_list, RMSE_day_list, NNmodel_list = retrainNNmodel_v2(models,comb_df,hyperParams,normInputData,normalizeFactorNN,filenameNNnew,filenameNNaux,testSetPercNN)\n",
    "    print('NN training: done in {} seconds'.format(np.round(time.time()-startTime)))\n",
    "    \n",
    "    startTime = time.time()\n",
    "    initPerc = 0.05 # use 5% of available data for SARIMA model initialization\n",
    "    initSize = int(initPerc*len(obsDf))\n",
    "    initSet, trainSet = obsDf.iloc[0:initSize], obsDf.iloc[initSize:len(obsDf)]\n",
    "    alpha = optimizeWeights_v2(models,SARIMAorder,SARIMAparams,initSet,trainSet,predHor,timeStep,\n",
    "                    tsPeriod,NNmodel,normInputData,normalizeFactorSARIMA,normalizeFactorNN,wf_df,filenameAlphaNew)\n",
    "    print('alpha training: done in {} seconds'.format(np.round(time.time()-startTime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainAlphaModel(models,obsDf,wf_df,filenameSARIMAnew,filenameNNnew,filenameNNaux,filenameAlphaNew,testSetPercNN,\n",
    "                         hyperParams={'hidden_layer_sizes': [(2),(3),(4),(5),(6),(7),(8),(9),(10),(20),(30),(40),(50),(60),(70),(80),(90),(100),(110),(120),\n",
    "                                                            (2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(10,10),(20,20),(30,30),(40,40),(50,50),(60,60),(70,70),(80,80),(90,90),(100,100),(110,110),(120,120)],\n",
    "                                      'activation': ['identity','relu','logistic','tanh'],\n",
    "                                      'solver': ['lbfgs','sgd','adam'],'learning_rate': ['constant','invscaling','adaptive'],'max_iter': [10000],\n",
    "                                      'early_stopping': [True], 'validation_fraction': [0.1]},\n",
    "                         predHor=96,timeStep=15,tsPeriod=96):\n",
    "    '''\n",
    "    Re-optimize weighting factors\n",
    "    \n",
    "    Inputs:\n",
    "    models: prediction models\n",
    "    obsDf: historical power dataframe (dataframe has only one data column)\n",
    "    wf_df: historical weather forecast dataframe\n",
    "    filenameSARIMAnew: filename to save newly trained SARIMA model\n",
    "    filenameNNnew: filename to save newly trained NN model\n",
    "    filenameAlphaNew: filename to save newly calculated alpha factors\n",
    "    testSetPercNN: proportion (from 0 to 1) of comb_df that is used for validation/testing\n",
    "    hyperParams: dictionary with the grid search for hyper parameters (the range of each hyper-parameter is given as a list).\n",
    "    predHor: prediction horizon (set to what is used in the MPC), in number of time steps\n",
    "    timeStep: time step of forecasted time series with SARIMA (in minutes)\n",
    "    tsPeriod: seasonality in SARIMA (in number of time steps)\n",
    "    '''\n",
    "      \n",
    "    SARIMAorder = models['sarima']['model']['SARIMAorder']\n",
    "    SARIMAparams = models['sarima']['model']['SARIMAparams']\n",
    "    normalizeFactorSARIMA = float(models['sarima']['normPowerCoeff'])\n",
    "    normalizeFactorNN = float(models['nn']['normPowerCoeff'])\n",
    "    normInputData = models['nn']['normInputData']    \n",
    "    NNmodel = models['nn']['model']\n",
    "    \n",
    "    startTime = time.time()\n",
    "    initPerc = 0.05 # use 5% of available data for SARIMA model initialization\n",
    "    initSize = int(initPerc*len(obsDf))\n",
    "    initSet, trainSet = obsDf.iloc[0:initSize], obsDf.iloc[initSize:len(obsDf)]\n",
    "    alpha = optimizeWeights_v2(models,SARIMAorder,SARIMAparams,initSet,trainSet,predHor,timeStep,\n",
    "                    tsPeriod,NNmodel,normInputData,normalizeFactorSARIMA,normalizeFactorNN,wf_df,filenameAlphaNew)\n",
    "    print('alpha training: done in {} seconds'.format(np.round(time.time()-startTime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrainNN(models,obsDf,wf_df,filenameNNnew,filenameNNaux,testSetPercNN,\n",
    "                         hyperParams={'hidden_layer_sizes': [(2),(3),(4),(5),(6),(7),(8),(9),(10),(20),(30),(40),(50),(60),(70),(80),(90),(100),(110),(120),\n",
    "                                                            (2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(10,10),(20,20),(30,30),(40,40),(50,50),(60,60),(70,70),(80,80),(90,90),(100,100),(110,110),(120,120)],\n",
    "                                      'activation': ['identity','relu','logistic','tanh'],\n",
    "                                      'solver': ['lbfgs','sgd','adam'],'learning_rate': ['constant','invscaling','adaptive'],'max_iter': [10000],\n",
    "                                      'early_stopping': [True], 'validation_fraction': [0.1]}):\n",
    "    '''   \n",
    "    Inputs:\n",
    "    models: prediction models\n",
    "    obsDf: historical power dataframe (dataframe has only one data column)\n",
    "    wf_df: historical weather forecast dataframe\n",
    "    filenameNNnew: filename to save newly trained NN model\n",
    "    testSetPercNN: proportion (from 0 to 1) of comb_df that is used for validation/testing\n",
    "    hyperParams: dictionary with the grid search for hyper parameters (the range of each hyper-parameter is given as a list).\n",
    "    '''\n",
    "        \n",
    "    # Retrain NN\n",
    "    startTime = time.time()\n",
    "    normalizeFactorNN = float(models['nn']['normPowerCoeff'])\n",
    "    normInputData = models['nn']['normInputData']\n",
    "    obsDf_NN = obsDf/normalizeFactorNN\n",
    "    aux = obsDf_NN.copy()\n",
    "    aux.index = pd.to_datetime(obsDf_NN.index)\n",
    "    obsDf_NN = aux.resample('60T',base=obsDf_NN.index[0].minute).mean()\n",
    "    obsDf_NN = obsDf_NN.dropna()\n",
    "    comb_df = makeCombinedDf_NN(wf_df,obsDf_NN)\n",
    "    NNmodel, RMSE_insample_list, RMSE_list, RMSE_day_list, NNmodel_list = retrainNNmodel_v2(models,comb_df,hyperParams,normInputData,normalizeFactorNN,filenameNNnew,filenameNNaux,testSetPercNN)\n",
    "    print('NN training: done in {} seconds'.format(np.round(time.time()-startTime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_NN_outofsample(models,testNN_df):\n",
    "    NNmodel = models['nn']['model']\n",
    "    predHor = int(len(testNN_df[testNN_df.columns[0]].iloc[0])) # NN prediction horizon\n",
    "    normOutputFact = float(models['nn']['normPowerCoeff'])\n",
    "    normInputData = models['nn']['normInputData']\n",
    "    \n",
    "    # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "    if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "        cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    \n",
    "    if models['nn']['architecture']=='scalar':\n",
    "        nrows = len(testNN_df)*predHor\n",
    "        ncolsX = int(len(np.concatenate(testNN_df[cols_df].iloc[0]))/float(predHor))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "        ncolsY = 1\n",
    "        Xf = np.empty([nrows,ncolsX])\n",
    "        Yf = np.empty([nrows,ncolsY])\n",
    "        Yr = np.empty([nrows,ncolsY])\n",
    "        idx = 0\n",
    "        for i in range(len(testNN_df)):\n",
    "            for j in range(predHor):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(testNN_df[k].iloc[i][j])]\n",
    "                if models['nn']['inputData'][-1]==True: tmp = tmp + [np.array(j+1)]\n",
    "                Xf[idx,:] = tmp\n",
    "                if normInputData: Xf[idx,:] = normalize(Xf[idx,:].reshape(1,-1))\n",
    "                Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                Yf[idx,:] = normOutputFact*Yf[idx,:] # de-normalize\n",
    "                Yr[idx,:] = normOutputFact*testNN_df['Observations'].iloc[i][j]\n",
    "                idx += 1\n",
    "    elif models['nn']['architecture']=='vector':\n",
    "        nrows = len(testNN_df)\n",
    "        ncolsX = len(np.concatenate(testNN_df[cols_df].iloc[0]))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=predHor\n",
    "        ncolsY = len(testNN_df['Observations'].iloc[0])\n",
    "        Xf = np.empty([nrows,ncolsX])\n",
    "        Yf = np.empty([nrows,ncolsY])\n",
    "        Yr = np.empty([nrows,ncolsY])\n",
    "        for i in range(nrows):\n",
    "            tmp = np.concatenate(testNN_df[cols_df].iloc[i])\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                tmp = np.concatenate((tmp ,np.arange(1,predHor+1)))\n",
    "            Xf[i,:] = tmp\n",
    "            if normInputData: Xf[i,:] = normalize(Xf[i,:].reshape(1,-1))\n",
    "            Yf[i,:] = NNmodel.predict(Xf[i,:].reshape(1,-1)) # NN forecasts\n",
    "            Yf[i,:] = normOutputFact*Yf[i,:] # de-normalize\n",
    "            Yr[i,:] = np.array(normOutputFact)*testNN_df['Observations'].iloc[i]\n",
    "    else:\n",
    "        raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                Use either scalar or vector')\n",
    "    RMSE = rmse(Yr,Yf)\n",
    "    daytime_idx = np.where(Yr>10) # larger than 10 Watt\n",
    "    RMSE_day = rmse(Yr[daytime_idx],Yf[daytime_idx])\n",
    "    \n",
    "    return RMSE, RMSE_day, Yr, Yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_NN_outofsample_v2(models,testNN_df):\n",
    "    NNmodel = models['nn']['model']\n",
    "    predHor = int(len(testNN_df[testNN_df.columns[0]].iloc[0])) # NN prediction horizon\n",
    "    normOutputFact = float(models['nn']['normPowerCoeff'])\n",
    "    normInputData = models['nn']['normInputData']\n",
    "    \n",
    "    # Select input data. Order: ambient temp, cloud cover, clear sky, Pdminus1, predHorizon\n",
    "    if models['nn']['inputData']==[False,True,True,False,False]:\n",
    "        cols_df = ['cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,False]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    elif models['nn']['inputData']==[True,True,True,False,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "    elif models['nn']['inputData']==[True,True,True,True,True]:\n",
    "        cols_df = ['Tamb_forecast','cloud_cover_forecast','clear_sky_forecast','Observations_dminus1']\n",
    "    \n",
    "    if models['nn']['architecture']=='scalar':\n",
    "        nrows = len(testNN_df)*predHor\n",
    "        ncolsX = int(len(np.concatenate(testNN_df[cols_df].iloc[0]))/float(predHor))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=1\n",
    "        ncolsY = 1\n",
    "        Xf = np.empty([nrows,ncolsX])\n",
    "        Yf = np.empty([nrows,ncolsY])\n",
    "        Yr = np.empty([nrows,ncolsY])\n",
    "        idx = 0\n",
    "        for i in range(len(testNN_df)):\n",
    "            for j in range(predHor):\n",
    "                tmp = []\n",
    "                for k in cols_df:\n",
    "                    tmp = tmp + [np.array(testNN_df[k].iloc[i][j])]\n",
    "                if models['nn']['inputData'][-1]==True: tmp = tmp + [np.array(j+1)]\n",
    "                Xf[idx,:] = tmp\n",
    "                if normInputData: \n",
    "                    Xf[idx,:] = normalizeInputNN(models,Xf[idx,:],cols_df,predHor)\n",
    "                Yf[idx,:] = NNmodel.predict(Xf[idx,:].reshape(1,-1)) # NN forecasts\n",
    "                Yf[idx,:] = normOutputFact*Yf[idx,:] # de-normalize\n",
    "                Yr[idx,:] = normOutputFact*testNN_df['Observations'].iloc[i][j]\n",
    "                idx += 1\n",
    "    elif models['nn']['architecture']=='vector':\n",
    "        nrows = len(testNN_df)\n",
    "        ncolsX = len(np.concatenate(testNN_df[cols_df].iloc[0]))\n",
    "        if models['nn']['inputData'][-1]==True: ncolsX+=predHor\n",
    "        ncolsY = len(testNN_df['Observations'].iloc[0])\n",
    "        Xf = np.empty([nrows,ncolsX])\n",
    "        Yf = np.empty([nrows,ncolsY])\n",
    "        Yr = np.empty([nrows,ncolsY])\n",
    "        for i in range(nrows):\n",
    "            tmp = np.concatenate(testNN_df[cols_df].iloc[i])\n",
    "            if models['nn']['inputData'][-1]==True:\n",
    "                tmp = np.concatenate((tmp ,np.arange(1,predHor+1)))\n",
    "            Xf[i,:] = tmp\n",
    "            if normInputData: \n",
    "                Xf[i,:] = normalizeInputNN(models,Xf[i,:],cols_df,predHor)\n",
    "            Yf[i,:] = NNmodel.predict(Xf[i,:].reshape(1,-1)) # NN forecasts\n",
    "            Yf[i,:] = normOutputFact*Yf[i,:] # de-normalize\n",
    "            Yr[i,:] = np.array(normOutputFact)*testNN_df['Observations'].iloc[i]\n",
    "    else:\n",
    "        raise ValueError('No appropriate selection for NN architecture! \\\n",
    "                Use either scalar or vector')\n",
    "    RMSE = rmse(Yr,Yf)\n",
    "    daytime_idx = np.where(Yr>10) # larger than 10 Watt\n",
    "    RMSE_day = rmse(Yr[daytime_idx],Yf[daytime_idx])\n",
    "    \n",
    "    return RMSE, RMSE_day, Yr, Yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load PV data\n",
    "def load_pv_data(pv_data_file):\n",
    "    df_pv = pd.read_csv(pv_data_file,index_col=[0])\n",
    "\n",
    "    if pv_data_file == 'PV_data_cleanedup_20180904.csv':\n",
    "        # Set index\n",
    "        df_pv['date'] = pd.to_datetime(df_pv['date'])\n",
    "        df_pv = df_pv.set_index('date')\n",
    "\n",
    "        # df_pv = df_pv['totPower'] # use this if total PV power is of interest\n",
    "        df_pv = df_pv['PV1_activePower_computed'] # use this if only inverter 1 PV power is of interest\n",
    "        \n",
    "        # resample for SARIMA\n",
    "        df_pv = df_pv.resample('15T',base=df_pv.index[0].minute).mean()\n",
    "        df_pv = df_pv.dropna()\n",
    "        tsPeriod = 96\n",
    "        \n",
    "        # resample for NN\n",
    "        df_pv_1h = df_pv.resample('60T',base=df_pv.index[0].minute).mean()\n",
    "        df_pv_1h = df_pv_1h.dropna()\n",
    "    else:\n",
    "        # set index\n",
    "        df_pv.index = pd.to_datetime(df_pv.index)\n",
    "        \n",
    "        # resample for SARIMA\n",
    "        df_pv = df_pv.resample('15T',base=df_pv.index[0].minute).mean()\n",
    "        df_pv = df_pv.dropna()\n",
    "        tsPeriod = 96\n",
    "        df_pv = df_pv['DC_PV_W']\n",
    "        \n",
    "        # resample for NN\n",
    "        df_pv_1h = df_pv.resample('60T',base=df_pv.index[0].minute).mean()\n",
    "        df_pv_1h = df_pv_1h.dropna()\n",
    "\n",
    "    return df_pv, df_pv_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wf_data(filename):\n",
    "    # Load weather forecast data\n",
    "    import csv\n",
    "    with open(filename, 'rb') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        data = {}\n",
    "        for row in reader:\n",
    "            row = ''.join(row)\n",
    "            if row[0:4] == '2018':\n",
    "                acq_date = row\n",
    "            if row[0] == '[':\n",
    "                if ((acq_date[5:7] != '07') | ((acq_date[5:7] == '07') & (int(acq_date[8:10]) >= 20))):\n",
    "                    data[acq_date] = eval(row.replace('][','],['))\n",
    "\n",
    "    # Keep only data tha correspond to full hours (to avoid redundant data)\n",
    "    wf_df_aux = data\n",
    "    for key in wf_df_aux.keys():\n",
    "        if pd.to_datetime(key).minute!=0:\n",
    "            wf_df_aux.pop(key, None)  \n",
    "    return wf_df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wf_data_to_df(filename,lbnl,surface_tilt,surface_azimuth,wf_df_aux,reSaveData):\n",
    "    # Transform weather data dict to a dataframe with [clear sky irrad, ambient temp, cloud cover] \n",
    "\n",
    "    if reSaveData:\n",
    "        timestampStringList = []\n",
    "        Tamb_list = []\n",
    "        cloudCover_list = []\n",
    "        clearSky_list = []\n",
    "        i = 0\n",
    "        start = time.time()\n",
    "        for key in wf_df_aux.keys():\n",
    "            for item in list(wf_df_aux[key][2:]): \n",
    "                if item[0] not in timestampStringList:\n",
    "                    timestampStringList = timestampStringList + [item[0]]\n",
    "        timestampList = sorted(pd.to_datetime(timestampStringList))\n",
    "        #timestamp = pd.DatetimeIndex(timestampList,tz=lbnl.tz) \n",
    "        timestamp = pd.DatetimeIndex(timestampList,tz=-3600*7) # PST is 7 hours behind UTC (lbnl object is in UTC)\n",
    "        cs = lbnl.get_clearsky(timestamp)\n",
    "        solarPos = lbnl.get_solarposition(timestamp)\n",
    "        totIrr = irrad.total_irrad(surface_tilt,surface_azimuth,solarPos.apparent_zenith,solarPos.azimuth,cs['dni'],cs['ghi'],cs['dhi'])\n",
    "        ghiPVTrain = totIrr['poa_global']\n",
    "\n",
    "        wfdTimestamp = []\n",
    "        for key in wf_df_aux.keys():\n",
    "            cur_Tamb_list = []\n",
    "            cur_cloudCover_list = []\n",
    "            cur_clearSky_list = []\n",
    "            wfdTimestamp = wfdTimestamp + [key]\n",
    "            print('Percent done: {}'.format(np.round(100*i/len(wf_df_aux.keys()),2)))\n",
    "            for item in list(wf_df_aux[key][2:]):   \n",
    "                pos = timestampList.index(pd.to_datetime(item[0]))\n",
    "                cur_Tamb_list = cur_Tamb_list + [item[1]]\n",
    "                cur_cloudCover_list = cur_cloudCover_list + [item[2]]\n",
    "                cur_clearSky_list = cur_clearSky_list + [ghiPVTrain.iloc[pos]]\n",
    "            Tamb_list = Tamb_list + [cur_Tamb_list]\n",
    "            cloudCover_list = cloudCover_list + [cur_cloudCover_list]\n",
    "            clearSky_list = clearSky_list + [cur_clearSky_list]\n",
    "            i+=1 \n",
    "        Tamb_list = np.array(Tamb_list)   \n",
    "        cloudCover_list = np.array(cloudCover_list)\n",
    "        clearSky_list = np.array(clearSky_list)\n",
    "        #np.savez('weather_forecast_data.npz',wfdTimestamp=wfdTimestamp,Tamb=Tamb_list,cloudCover=cloudCover_list,clearSky=clearSky_list)\n",
    "\n",
    "        # Save sorted data in a df\n",
    "        wf_df = pd.DataFrame(data=[np.array(wfdTimestamp), np.array(Tamb_list), np.array(cloudCover_list), np.array(clearSky_list)]).transpose()\n",
    "        wf_df.columns = ['timestamp','Tamb_forecast','cloud_cover_forecast','clear_sky_forecast']\n",
    "        aux = np.array(wf_df['timestamp'])\n",
    "        wf_df = wf_df.set_index(pd.to_datetime(wf_df['timestamp']))\n",
    "        wf_df = wf_df.iloc[:,[1,2,3]]\n",
    "        wf_df['timeIndex'] = aux\n",
    "        wf_df = wf_df.sort_values(by='timeIndex')\n",
    "        wf_df.to_json(filename)\n",
    "\n",
    "        timeelapsed = time.time()-start\n",
    "        print('Time elapsed: {}'.format(timeelapsed))\n",
    "        \n",
    "        return wf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optSARIMAstruct(train,tsPeriod,listOfOrdersToTry,filename,listOfParamsWS=None):\n",
    "    \"\"\"\n",
    "    Iterate of a list of possible SARIMA model structures and compute in-sample RMSEs for 1-step ahead prediction\n",
    "    \n",
    "    train: training set (dataframe column that corresponds to total PV power)\n",
    "    tsPeriod: seasonality in SARIMA\n",
    "    listOfOrdersToTry: list of SARIMA structures to be checked in the form [p,d,q,P,D,Q]\n",
    "    listOfParamsWS: list of initial guesses of optimal SARIMA coefficients from previous runs (WS: Warm Start)\n",
    "    filename: file to save the results\n",
    "    \"\"\"\n",
    "    \n",
    "    listOfRMSEs = []\n",
    "    listOfOptTimes = []\n",
    "    listOfOrders = []\n",
    "    listOfParams = []\n",
    "    if listOfParamsWS is not None: params = listOfParamsWS\n",
    "    totComb = len(listOfOrdersToTry)\n",
    "    cnt = 0\n",
    "    successCnt = 0\n",
    "    \n",
    "    for i in range(len(listOfOrdersToTry)):\n",
    "        curList = listOfOrdersToTry[i]\n",
    "        p = int(curList[0])\n",
    "        d = int(curList[1])\n",
    "        q = int(curList[2])\n",
    "        P = int(curList[3])\n",
    "        D = int(curList[4])\n",
    "        Q = int(curList[5])\n",
    "        cnt += 1\n",
    "        \n",
    "        try:\n",
    "            startTime = time.time()\n",
    "            print('Fitting model: {}'.format([p,d,q,P,D,Q]))\n",
    "            model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, tsPeriod))\n",
    "            if listOfParamsWS is not None:\n",
    "                model_fit = model.fit(disp=0, method='lbfgs',start_params=params[i])\n",
    "            else:\n",
    "                model_fit = model.fit(disp=0, method='lbfgs')\n",
    "            listOfRMSEs.append(rmse(model_fit.fittedvalues,train))\n",
    "            listOfOrders.append([p,d,q,P,D,Q])\n",
    "            successCnt += 1\n",
    "            listOfParams.append(model_fit.params.get_values().tolist())\n",
    "            optTime = time.time() - startTime\n",
    "            listOfOptTimes.append(optTime)\n",
    "            print('Optimization time was {} seconds. Success ratio is {}%'.format(np.round(optTime,2),np.round(100*(successCnt/cnt))))\n",
    "        except:\n",
    "            print('Failed!')\n",
    "        print('Completed: {}%'.format(np.round(100*(cnt/totComb),2)))\n",
    "    \n",
    "    res = {'listOfRMSEs':listOfRMSEs, 'listOfOrders':listOfOrders, 'listOfOptTimes':listOfOptTimes, 'listOfParams':listOfParams}\n",
    "    with open(filename+'.json', 'wb') as f:\n",
    "        json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findBestSARIMAOutOfSampleMultipleSteps(filename1, filename2, trainSet, testSet, tsPeriod, predHor, timeStep,\n",
    "                                           retrainFlag, numModels):\n",
    "    \"\"\"\n",
    "    Load result from grid search and identify the best SARIMA models in terms of out-of-sample performance\n",
    "    Multiple-step ahead forecasts are considered\n",
    "\n",
    "    filename1: file with saved results from 1-step ahead out-of-sample analysis\n",
    "    filename2: file to save results from multiple-step ahead out-of-sample analysis\n",
    "    trainSet: training set (dataframe column)\n",
    "    testSet: test set (dataframe column)\n",
    "    tsPeriod: seasonality in SARIMA\n",
    "    predHor: prediction horizon as # of timeStep (set to what used in the MPC)\n",
    "    timeStep: time step of forecasted time series (in minutes)\n",
    "    retrainFlag: If True, a new SARIMA model is fit every time a new prediction is needed within the prediction horizon\n",
    "    numModels: the number of top-performing models to be selected\n",
    "    \"\"\"\n",
    "\n",
    "    print('=============== {}-step ahead, out-of-sample Results ==============='.format(predHor))\n",
    "    with open(filename1,'rb') as f:\n",
    "        data = json.load(f)\n",
    "    listOfOrdersSaved = np.array(data['listOfOrders'])\n",
    "    listOfRMSEsSaved = np.array(data['listOfRMSEs'])\n",
    "    listOfParamsSaved = data['listOfParams']\n",
    "\n",
    "    listOfRMSEsSaved = np.array(listOfRMSEsSaved)\n",
    "    ordered = np.argsort(listOfRMSEsSaved)\n",
    "    listOfOrdersSavedRanked = listOfOrdersSaved[ordered[:]]\n",
    "    listOfParamsSavedRanked = []\n",
    "    for i in ordered: \n",
    "        listOfParamsSavedRanked = listOfParamsSavedRanked + [listOfParamsSaved[i]]\n",
    "    listOfRMSEsSavedRanked = listOfRMSEsSaved[ordered[:]]\n",
    "    \n",
    "    listOfRMSEsComputed = []\n",
    "    numModels = np.amin([numModels, len(listOfParamsSaved)])\n",
    "    simHor = len(testSet) - predHor\n",
    "    totComb = numModels\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(numModels):\n",
    "        curOrder = listOfOrdersSavedRanked[i]\n",
    "        curParams = np.array(listOfParamsSavedRanked[i])\n",
    "        predMat = []\n",
    "        obsMat = []\n",
    "        obsDf = trainSet\n",
    "        print(curOrder)\n",
    "\n",
    "        maxSeasLag = np.amax([curOrder[2],curOrder[5]])\n",
    "        maxLag = np.amax([1, maxSeasLag])\n",
    "        curModel = SARIMAX(trainSet[-maxLag*predHor-1:], order=(int(curOrder[0]), int(curOrder[1]), int(curOrder[2])),\n",
    "                           seasonal_order=(int(curOrder[3]), int(curOrder[4]), int(curOrder[5]), tsPeriod))\n",
    "        startTime = time.time()\n",
    "        curModelFit = curModel.filter(curParams)\n",
    "        print('It took {} seconds to build filter'.format(time.time() - startTime))\n",
    "\n",
    "        try:\n",
    "            for t in range(simHor):\n",
    "                #if (testSet.index[t].hour == 6) & (testSet.index[t].minute == 0) & (len(np.where(obsDf.isnull().iloc[-maxLag*predHor-1:])[0])==0) & (testSet.isnull().iloc[t:t + predHor].any() == False):  # | (testSet.index[t].hour==12):\n",
    "                if (testSet.index[t].minute == 0) & (len(np.where(obsDf.isnull().iloc[-maxLag * predHor - 1:])[0]) == 0) & (testSet.isnull().iloc[t:t + predHor].any() == False):                \n",
    "                    # Get predictions\n",
    "                    if retrainFlag:\n",
    "                        curPred = multiStepSARIMAforecast_withRetrain(obsDf.iloc[-maxLag*predHor-1:], curModelFit, predHor, timeStep, retrainFlag)\n",
    "                    else:\n",
    "                        curPred = multiStepSARIMAforecast(obsDf.iloc[-maxLag*predHor-1:], curModelFit, predHor)\n",
    "                    predMat = predMat + [curPred.tolist()]\n",
    "                    # Collect observations\n",
    "                    obsMat = obsMat + [testSet.iloc[t:t + predHor].values]\n",
    "\n",
    "                # Update 'observation' dataframe\n",
    "                obs = pd.DataFrame(data=np.array([testSet.iloc[t]]),\n",
    "                                   index=[obsDf.index[-1] + pd.Timedelta(minutes=timeStep)])\n",
    "                obsDf = obsDf.append(obs)\n",
    "            predMat = np.array(predMat)\n",
    "            obsMat = np.array(obsMat)\n",
    "            predVec = np.reshape(predMat, (predMat.shape[0] * predMat.shape[1], 1))\n",
    "            obsVec = np.reshape(obsMat, (obsMat.shape[0] * obsMat.shape[1], 1))\n",
    "            listOfRMSEsComputed.append(rmse(predVec, obsVec))\n",
    "        except:\n",
    "            listOfRMSEsComputed.append(np.inf)\n",
    "        cnt += 1\n",
    "        print('Completed: {}%'.format(np.round(100 * (cnt / totComb), 2)))\n",
    "\n",
    "    listOfRMSEsComputed = np.array(listOfRMSEsComputed)\n",
    "    ordered = np.argsort(listOfRMSEsComputed)\n",
    "    bestNordersList = listOfOrdersSavedRanked[ordered[0:numModels]]\n",
    "    bestNparamsList = []\n",
    "    for jj in ordered[0:numModels]:\n",
    "        bestNparamsList = bestNparamsList + [listOfParamsSavedRanked[jj]]\n",
    "    bestNRMSEsList = listOfRMSEsComputed[ordered[0:numModels]]\n",
    "    print('The best {} model orders are:'.format(numModels))\n",
    "    print(listOfOrdersSavedRanked[ordered[0:numModels]])\n",
    "    print('The best {} RMSEs are:'.format(numModels))\n",
    "    print(listOfRMSEsComputed[ordered[0:numModels]])\n",
    "\n",
    "    bestOrder = listOfOrdersSavedRanked[ordered[0]]\n",
    "    bestParams = listOfParamsSavedRanked[ordered[0]]\n",
    "    minRMSE = listOfRMSEsComputed[ordered[0]]\n",
    "    bestModel = SARIMAX(trainSet, order=(int(bestOrder[0]), int(bestOrder[1]), int(bestOrder[2])),\n",
    "                        seasonal_order=(int(bestOrder[3]), int(bestOrder[4]), int(bestOrder[5]), tsPeriod))\n",
    "    bestModelFit = bestModel.filter(bestParams)\n",
    "\n",
    "    print('Results for the best model')\n",
    "    print(bestModelFit.summary())\n",
    "    print('\\nMinumum out-of-sample nRMSE for {}-step prediction is {}'.format(predHor, minRMSE))\n",
    "    \n",
    "    ordersListRes = bestNordersList.tolist()\n",
    "    paramsListRes = []\n",
    "    for elem in bestNparamsList: \n",
    "        paramsListRes = paramsListRes + [elem]\n",
    "    RMSEsListRes = bestNRMSEsList.tolist()\n",
    "    res = {'listOfOrders':ordersListRes, 'listOfParams':paramsListRes, 'listOfRMSEs':RMSEsListRes}\n",
    "    with open(filename2+'.json', 'wb') as f:\n",
    "        json.dump(res, f)\n",
    "\n",
    "    return bestModel, bestModelFit, bestOrder, bestParams, bestNordersList, bestNparamsList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
